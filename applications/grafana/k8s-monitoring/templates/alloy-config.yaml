---
# Source: k8s-monitoring/templates/alloy-config.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: grafana-k8s-monitoring-alloy-metrics
  namespace: grafana
data:
  config.alloy: |
    // Feature: Annotation Autodiscovery
    declare "annotation_autodiscovery" {
      argument "metrics_destinations" {
        comment = "Must be a list of metric destinations where collected metrics should be forwarded to"
      }
    
      discovery.kubernetes "pods" {
        role = "pod"
      }
    
      discovery.relabel "annotation_autodiscovery_pods" {
        targets = discovery.kubernetes.pods.targets
        rule {
          source_labels = ["__meta_kubernetes_pod_annotation_k8s_grafana_com_scrape"]
          regex = "true"
          action = "keep"
        }
        // Only keep pods that are running, ready, and not init containers.
        rule {
          source_labels = [
            "__meta_kubernetes_pod_phase",
            "__meta_kubernetes_pod_ready",
            "__meta_kubernetes_pod_container_init",
          ]
          regex = "Running;true;false"
          action = "keep"
        }
        rule {
          source_labels = ["__meta_kubernetes_pod_name"]
          target_label = "pod"
        }
        rule {
          source_labels = ["__meta_kubernetes_pod_container_name"]
          target_label = "container"
        }
        rule {
          source_labels = ["__meta_kubernetes_namespace"]
          target_label = "namespace"
        }
        rule {
          source_labels = ["__meta_kubernetes_pod_annotation_k8s_grafana_com_job"]
          target_label = "job"
        }
        rule {
          source_labels = ["job", "__meta_kubernetes_pod_label_app_kubernetes_io_name"]
          regex = ";(.+)"
          target_label = "job"
        }
        rule {
          source_labels = ["job", "__meta_kubernetes_pod_label_app"]
          regex = ";(.+)"
          target_label = "job"
        }
        rule {
          source_labels = ["job", "container"]
          regex = ";(.+)"
          target_label = "job"
        }
        rule {
          source_labels = ["__meta_kubernetes_pod_annotation_k8s_grafana_com_instance"]
          target_label = "instance"
        }
    
        // Rules to choose the right container
        rule {
          source_labels = ["container"]
          target_label = "__tmp_container"
        }
        rule {
          source_labels = ["__meta_kubernetes_pod_annotation_k8s_grafana_com_metrics_container"]
          regex = "(.+)"
          target_label = "__tmp_container"
        }
        rule {
          source_labels = ["container"]
          action = "keepequal"
          target_label = "__tmp_container"
        }
        rule {
          action = "labeldrop"
          regex = "__tmp_container"
        }
    
        // Set metrics path
        rule {
          source_labels = ["__meta_kubernetes_pod_annotation_k8s_grafana_com_metrics_path"]
          regex = "(.+)"
          target_label = "__metrics_path__"
        }
    
        // Set metrics scraping URL parameters
        rule {
          action = "labelmap"
          regex = "__meta_kubernetes_pod_annotation_k8s_grafana_com_metrics_param_(.+)"
          replacement = "__param_$1"
        }
    
        // Choose the pod port
        // The discovery generates a target for each declared container port of the pod.
        // If the metricsPortName annotation has value, keep only the target where the port name matches the one of the annotation.
        rule {
          source_labels = ["__meta_kubernetes_pod_container_port_name"]
          target_label = "__tmp_port"
        }
        rule {
          source_labels = ["__meta_kubernetes_pod_annotation_k8s_grafana_com_metrics_portName"]
          regex = "(.+)"
          target_label = "__tmp_port"
        }
        rule {
          source_labels = ["__meta_kubernetes_pod_container_port_name"]
          action = "keepequal"
          target_label = "__tmp_port"
        }
        rule {
          action = "labeldrop"
          regex = "__tmp_port"
        }
    
        // If the metrics port number annotation has a value, override the target address to use it, regardless whether it is
        // one of the declared ports on that Pod.
        rule {
          source_labels = ["__meta_kubernetes_pod_annotation_k8s_grafana_com_metrics_portNumber", "__meta_kubernetes_pod_ip"]
          regex = "(\\d+);(([A-Fa-f0-9]{1,4}::?){1,7}[A-Fa-f0-9]{1,4})"
          replacement = "[$2]:$1" // IPv6
          target_label = "__address__"
        }
        rule {
          source_labels = ["__meta_kubernetes_pod_annotation_k8s_grafana_com_metrics_portNumber", "__meta_kubernetes_pod_ip"]
          regex = "(\\d+);((([0-9]+?)(\\.|$)){4})" // IPv4, takes priority over IPv6 when both exists
          replacement = "$2:$1"
          target_label = "__address__"
        }
    
        rule {
          source_labels = ["__meta_kubernetes_pod_annotation_k8s_grafana_com_metrics_scheme"]
          regex = "(.+)"
          target_label = "__scheme__"
        }
    
        rule {
          source_labels = ["__meta_kubernetes_pod_annotation_k8s_grafana_com_metrics_scrapeInterval"]
          regex = "(.+)"
          target_label = "__scrape_interval__"
        }
        rule {
          source_labels = ["__scrape_interval__"]
          regex = ""
          replacement = "60s"
          target_label = "__scrape_interval__"
        }
        rule {
          source_labels = ["__meta_kubernetes_pod_annotation_k8s_grafana_com_metrics_scrapeTimeout"]
          regex = "(.+)"
          target_label = "__scrape_timeout__"
        }
        rule {
          source_labels = ["__scrape_timeout__"]
          regex = ""
          replacement = "10s"
          target_label = "__scrape_timeout__"
        }
      }
    
      discovery.kubernetes "services" {
        role = "service"
      }
    
      discovery.relabel "annotation_autodiscovery_services" {
        targets = discovery.kubernetes.services.targets
        rule {
          source_labels = ["__meta_kubernetes_service_annotation_k8s_grafana_com_scrape"]
          regex = "true"
          action = "keep"
        }
        rule {
          source_labels = ["__meta_kubernetes_service_name"]
          target_label = "service"
        }
        rule {
          source_labels = ["__meta_kubernetes_namespace"]
          target_label = "namespace"
        }
        rule {
          source_labels = ["__meta_kubernetes_service_annotation_k8s_grafana_com_job"]
          target_label = "job"
        }
        rule {
          source_labels = ["job", "__meta_kubernetes_service_label_app_kubernetes_io_name"]
          regex = ";(.+)"
          target_label = "job"
        }
        rule {
          source_labels = ["job", "__meta_kubernetes_service_label_app"]
          regex = ";(.+)"
          target_label = "job"
        }
        rule {
          source_labels = ["job", "service"]
          regex = ";(.+)"
          target_label = "job"
        }
        rule {
          source_labels = ["__meta_kubernetes_service_annotation_k8s_grafana_com_instance"]
          target_label = "instance"
        }
    
        // Set metrics path
        rule {
          source_labels = ["__meta_kubernetes_service_annotation_k8s_grafana_com_metrics_path"]
          target_label = "__metrics_path__"
        }
    
        // Set metrics scraping URL parameters
        rule {
          action = "labelmap"
          regex = "__meta_kubernetes_service_annotation_k8s_grafana_com_metrics_param_(.+)"
          replacement = "__param_$1"
        }
    
        // Choose the service port
        rule {
          source_labels = ["__meta_kubernetes_service_port_name"]
          target_label = "__tmp_port"
        }
        rule {
          source_labels = ["__meta_kubernetes_service_annotation_k8s_grafana_com_metrics_portName"]
          regex = "(.+)"
          target_label = "__tmp_port"
        }
        rule {
          source_labels = ["__meta_kubernetes_service_port_name"]
          action = "keepequal"
          target_label = "__tmp_port"
        }
    
        rule {
          source_labels = ["__meta_kubernetes_service_port_number"]
          target_label = "__tmp_port"
        }
        rule {
          source_labels = ["__meta_kubernetes_service_annotation_k8s_grafana_com_metrics_portNumber"]
          regex = "(.+)"
          target_label = "__tmp_port"
        }
        rule {
          source_labels = ["__meta_kubernetes_service_port_number"]
          action = "keepequal"
          target_label = "__tmp_port"
        }
        rule {
          action = "labeldrop"
          regex = "__tmp_port"
        }
    
        rule {
          source_labels = ["__meta_kubernetes_service_annotation_k8s_grafana_com_metrics_scheme"]
          regex = "(.+)"
          target_label = "__scheme__"
        }
    
        rule {
          source_labels = ["__meta_kubernetes_service_annotation_k8s_grafana_com_metrics_scrapeInterval"]
          regex = "(.+)"
          target_label = "__scrape_interval__"
        }
        rule {
          source_labels = ["__scrape_interval__"]
          regex = ""
          replacement = "60s"
          target_label = "__scrape_interval__"
        }
        rule {
          source_labels = ["__meta_kubernetes_service_annotation_k8s_grafana_com_metrics_scrapeTimeout"]
          regex = "(.+)"
          target_label = "__scrape_timeout__"
        }
        rule {
          source_labels = ["__scrape_timeout__"]
          regex = ""
          replacement = "10s"
          target_label = "__scrape_timeout__"
        }
      }
    
      discovery.relabel "annotation_autodiscovery_http" {
        targets = array.concat(discovery.relabel.annotation_autodiscovery_pods.output, discovery.relabel.annotation_autodiscovery_services.output)
        rule {
          source_labels = ["__scheme__"]
          regex = "https"
          action = "drop"
        }
      }
    
      discovery.relabel "annotation_autodiscovery_https" {
        targets = array.concat(discovery.relabel.annotation_autodiscovery_pods.output, discovery.relabel.annotation_autodiscovery_services.output)
        rule {
          source_labels = ["__scheme__"]
          regex = "https"
          action = "keep"
        }
      }
    
      prometheus.scrape "annotation_autodiscovery_http" {
        targets = discovery.relabel.annotation_autodiscovery_http.output
        honor_labels = true
        bearer_token_file = "/var/run/secrets/kubernetes.io/serviceaccount/token"
        scrape_protocols = ["OpenMetricsText1.0.0","OpenMetricsText0.0.1","PrometheusText0.0.4"]
        scrape_classic_histograms = false
        clustering {
          enabled = true
        }
    
        forward_to = argument.metrics_destinations.value
      }
    
      prometheus.scrape "annotation_autodiscovery_https" {
        targets = discovery.relabel.annotation_autodiscovery_https.output
        honor_labels = true
        bearer_token_file = "/var/run/secrets/kubernetes.io/serviceaccount/token"
        tls_config {
          insecure_skip_verify = true
        }
        scrape_protocols = ["OpenMetricsText1.0.0","OpenMetricsText0.0.1","PrometheusText0.0.4"]
        scrape_classic_histograms = false
        clustering {
          enabled = true
        }
    
        forward_to = argument.metrics_destinations.value
      }
    }
    annotation_autodiscovery "feature" {
      metrics_destinations = [
        prometheus.remote_write.grafana_cloud_metrics.receiver,
      ]
    }
    // Feature: Auto-Instrumentation
    declare "auto_instrumentation" {
      argument "metrics_destinations" {
        comment = "Must be a list of metric destinations where collected metrics should be forwarded to"
      }
    
      discovery.kubernetes "beyla_pods" {
        role = "pod"
        namespaces {
          own_namespace = true
        }
        selectors {
          role = "pod"
          label = "app.kubernetes.io/name=beyla"
        }
      }
    
      discovery.relabel "beyla_pods" {
        targets = discovery.kubernetes.beyla_pods.targets
        rule {
          source_labels = ["__meta_kubernetes_pod_node_name"]
          action = "replace"
          target_label = "instance"
        }
      }
    
      prometheus.scrape "beyla_applications" {
        targets         = discovery.relabel.beyla_pods.output
        honor_labels    = true
        scrape_interval = "60s"
        scrape_protocols = ["OpenMetricsText1.0.0","OpenMetricsText0.0.1","PrometheusText0.0.4"]
        scrape_classic_histograms = false
        clustering {
          enabled = true
        }
        forward_to = argument.metrics_destinations.value
      }
    
      prometheus.scrape "beyla_internal" {
        targets         = discovery.relabel.beyla_pods.output
        metrics_path    = "/internal/metrics"
        job_name        = "integrations/beyla"
        honor_labels    = true
        scrape_interval = "60s"
        scrape_protocols = ["OpenMetricsText1.0.0","OpenMetricsText0.0.1","PrometheusText0.0.4"]
        scrape_classic_histograms = false
        clustering {
          enabled = true
        }
        forward_to = argument.metrics_destinations.value
      }
    }
    auto_instrumentation "feature" {
      metrics_destinations = [
        prometheus.remote_write.grafana_cloud_metrics.receiver,
      ]
    }
    // Feature: Cluster Metrics
    declare "cluster_metrics" {
      argument "metrics_destinations" {
        comment = "Must be a list of metric destinations where collected metrics should be forwarded to"
      }
      discovery.kubernetes "nodes" {
        role = "node"
      }
    
      discovery.relabel "nodes" {
        targets = discovery.kubernetes.nodes.targets
        rule {
          source_labels = ["__meta_kubernetes_node_name"]
          target_label  = "node"
        }
    
        rule {
          replacement = "kubernetes"
          target_label = "source"
        }
    
      }
    
      // Kubelet
      discovery.relabel "kubelet" {
        targets = discovery.relabel.nodes.output
      }
    
      prometheus.scrape "kubelet" {
        targets  = discovery.relabel.kubelet.output
        job_name = "integrations/kubernetes/kubelet"
        scheme   = "https"
        scrape_interval = "60s"
        scrape_protocols = ["OpenMetricsText1.0.0","OpenMetricsText0.0.1","PrometheusText0.0.4"]
        scrape_classic_histograms = false
        bearer_token_file = "/var/run/secrets/kubernetes.io/serviceaccount/token"
    
        tls_config {
          ca_file = "/var/run/secrets/kubernetes.io/serviceaccount/ca.crt"
          insecure_skip_verify = true
          server_name = "kubernetes"
        }
    
        clustering {
          enabled = true
        }
    
        forward_to = [prometheus.relabel.kubelet.receiver]
      }
    
      prometheus.relabel "kubelet" {
        max_cache_size = 100000
        rule {
          source_labels = ["__name__"]
          regex = "up|scrape_samples_scraped|go_goroutines|kubelet_certificate_manager_client_expiration_renew_errors|kubelet_certificate_manager_client_ttl_seconds|kubelet_certificate_manager_server_ttl_seconds|kubelet_cgroup_manager_duration_seconds_bucket|kubelet_cgroup_manager_duration_seconds_count|kubelet_node_config_error|kubelet_node_name|kubelet_pleg_relist_duration_seconds_bucket|kubelet_pleg_relist_duration_seconds_count|kubelet_pleg_relist_interval_seconds_bucket|kubelet_pod_start_duration_seconds_bucket|kubelet_pod_start_duration_seconds_count|kubelet_pod_worker_duration_seconds_bucket|kubelet_pod_worker_duration_seconds_count|kubelet_running_container_count|kubelet_running_containers|kubelet_running_pod_count|kubelet_running_pods|kubelet_runtime_operations_errors_total|kubelet_runtime_operations_total|kubelet_server_expiration_renew_errors|kubelet_volume_stats_available_bytes|kubelet_volume_stats_capacity_bytes|kubelet_volume_stats_inodes|kubelet_volume_stats_inodes_free|kubelet_volume_stats_inodes_used|kubelet_volume_stats_used_bytes|kubernetes_build_info|namespace_workload_pod|process_cpu_seconds_total|process_resident_memory_bytes|rest_client_requests_total|storage_operation_duration_seconds_count|storage_operation_errors_total|volume_manager_total_volumes"
          action = "keep"
        }
    
        forward_to = argument.metrics_destinations.value
      }
    
      // Kubelet Resources
      discovery.relabel "kubelet_resources" {
        targets = discovery.relabel.nodes.output
        rule {
          replacement   = "/metrics/resource"
          target_label  = "__metrics_path__"
        }
      }
    
      prometheus.scrape "kubelet_resources" {
        targets = discovery.relabel.kubelet_resources.output
        job_name = "integrations/kubernetes/resources"
        scheme   = "https"
        scrape_interval = "60s"
        scrape_protocols = ["OpenMetricsText1.0.0","OpenMetricsText0.0.1","PrometheusText0.0.4"]
        scrape_classic_histograms = false
        bearer_token_file = "/var/run/secrets/kubernetes.io/serviceaccount/token"
    
        tls_config {
          ca_file = "/var/run/secrets/kubernetes.io/serviceaccount/ca.crt"
          insecure_skip_verify = true
          server_name = "kubernetes"
        }
    
        clustering {
          enabled = true
        }
    
        forward_to = [prometheus.relabel.kubelet_resources.receiver]
      }
    
      prometheus.relabel "kubelet_resources" {
        max_cache_size = 100000
        rule {
          source_labels = ["__name__"]
          regex = "up|scrape_samples_scraped|node_cpu_usage_seconds_total|node_memory_working_set_bytes"
          action = "keep"
        }
    
        forward_to = argument.metrics_destinations.value
      }
    
      // cAdvisor
      discovery.relabel "cadvisor" {
        targets = discovery.relabel.nodes.output
        rule {
          replacement   = "/metrics/cadvisor"
          target_label  = "__metrics_path__"
        }
      }
    
      prometheus.scrape "cadvisor" {
        targets = discovery.relabel.cadvisor.output
        job_name = "integrations/kubernetes/cadvisor"
        scheme = "https"
        scrape_interval = "60s"
        scrape_protocols = ["OpenMetricsText1.0.0","OpenMetricsText0.0.1","PrometheusText0.0.4"]
        scrape_classic_histograms = false
        bearer_token_file = "/var/run/secrets/kubernetes.io/serviceaccount/token"
    
        tls_config {
          ca_file = "/var/run/secrets/kubernetes.io/serviceaccount/ca.crt"
          insecure_skip_verify = true
          server_name = "kubernetes"
        }
    
        clustering {
          enabled = true
        }
    
        forward_to = [prometheus.relabel.cadvisor.receiver]
      }
    
      prometheus.relabel "cadvisor" {
        max_cache_size = 100000
        rule {
          source_labels = ["__name__"]
          regex = "up|scrape_samples_scraped|container_cpu_cfs_periods_total|container_cpu_cfs_throttled_periods_total|container_cpu_usage_seconds_total|container_fs_reads_bytes_total|container_fs_reads_total|container_fs_writes_bytes_total|container_fs_writes_total|container_memory_cache|container_memory_rss|container_memory_swap|container_memory_usage_bytes|container_memory_working_set_bytes|container_network_receive_bytes_total|container_network_receive_packets_dropped_total|container_network_receive_packets_total|container_network_transmit_bytes_total|container_network_transmit_packets_dropped_total|container_network_transmit_packets_total|machine_memory_bytes"
          action = "keep"
        }
        // Drop empty container labels, addressing https://github.com/google/cadvisor/issues/2688
        rule {
          source_labels = ["__name__","container"]
          separator = "@"
          regex = "(container_cpu_.*|container_fs_.*|container_memory_.*)@"
          action = "drop"
        }
        // Drop empty image labels, addressing https://github.com/google/cadvisor/issues/2688
        rule {
          source_labels = ["__name__","image"]
          separator = "@"
          regex = "(container_cpu_.*|container_fs_.*|container_memory_.*|container_network_.*)@"
          action = "drop"
        }
        // Normalizing unimportant labels (not deleting to continue satisfying <label>!="" checks)
        rule {
          source_labels = ["__name__", "boot_id"]
          separator = "@"
          regex = "machine_memory_bytes@.*"
          target_label = "boot_id"
          replacement = "NA"
        }
        rule {
          source_labels = ["__name__", "system_uuid"]
          separator = "@"
          regex = "machine_memory_bytes@.*"
          target_label = "system_uuid"
          replacement = "NA"
        }
        // Filter out non-physical devices/interfaces
        rule {
          source_labels = ["__name__", "device"]
          separator = "@"
          regex = "container_fs_.*@(/dev/)?(mmcblk.p.+|nvme.+|rbd.+|sd.+|vd.+|xvd.+|dasd.+)"
          target_label = "__keepme"
          replacement = "1"
        }
        rule {
          source_labels = ["__name__", "__keepme"]
          separator = "@"
          regex = "container_fs_.*@"
          action = "drop"
        }
        rule {
          source_labels = ["__name__"]
          regex = "container_fs_.*"
          target_label = "__keepme"
          replacement = ""
        }
        rule {
          source_labels = ["__name__", "interface"]
          separator = "@"
          regex = "container_network_.*@(en[ospx][0-9].*|wlan[0-9].*|eth[0-9].*)"
          target_label = "__keepme"
          replacement = "1"
        }
        rule {
          source_labels = ["__name__", "__keepme"]
          separator = "@"
          regex = "container_network_.*@"
          action = "drop"
        }
        rule {
          source_labels = ["__name__"]
          regex = "container_network_.*"
          target_label = "__keepme"
          replacement = ""
        }
        forward_to = argument.metrics_destinations.value
      }
      discovery.kubernetes "kube_state_metrics" {
        role = "endpoints"
    
        selectors {
          role = "endpoints"
          label = "app.kubernetes.io/name=kube-state-metrics,release=grafana-k8s-monitoring"
        }
        namespaces {
          names = ["default"]
        }
      }
    
      discovery.relabel "kube_state_metrics" {
        targets = discovery.kubernetes.kube_state_metrics.targets
    
        // only keep targets with a matching port name
        rule {
          source_labels = ["__meta_kubernetes_pod_container_port_name"]
          regex = "http"
          action = "keep"
        }
    
        rule {
          action = "replace"
          replacement = "kubernetes"
          target_label = "source"
        }
    
      }
    
      prometheus.scrape "kube_state_metrics" {
        targets = discovery.relabel.kube_state_metrics.output
        job_name = "integrations/kubernetes/kube-state-metrics"
        scrape_interval = "60s"
        scrape_protocols = ["OpenMetricsText1.0.0","OpenMetricsText0.0.1","PrometheusText0.0.4"]
        scrape_classic_histograms = false
        scheme = "http"
        bearer_token_file = ""
        tls_config {
          insecure_skip_verify = true
        }
    
        clustering {
          enabled = true
        }
        forward_to = [prometheus.relabel.kube_state_metrics.receiver]
      }
    
      prometheus.relabel "kube_state_metrics" {
        max_cache_size = 100000
        rule {
          source_labels = ["__name__"]
          regex = "up|scrape_samples_scraped|kube_configmap_info|kube_configmap_metadata_resource_version|kube_cronjob.*|kube_daemonset.*|kube_deployment_metadata_generation|kube_deployment_spec_replicas|kube_deployment_status_condition|kube_deployment_status_observed_generation|kube_deployment_status_replicas_available|kube_deployment_status_replicas_updated|kube_horizontalpodautoscaler_spec_max_replicas|kube_horizontalpodautoscaler_spec_min_replicas|kube_horizontalpodautoscaler_status_current_replicas|kube_horizontalpodautoscaler_status_desired_replicas|kube_job.*|kube_namespace_status_phase|kube_node.*|kube_persistentvolume_status_phase|kube_persistentvolumeclaim_access_mode|kube_persistentvolumeclaim_info|kube_persistentvolumeclaim_labels|kube_persistentvolumeclaim_resource_requests_storage_bytes|kube_persistentvolumeclaim_status_phase|kube_pod_completion_time|kube_pod_container_info|kube_pod_container_resource_limits|kube_pod_container_resource_requests|kube_pod_container_status_last_terminated_reason|kube_pod_container_status_last_terminated_timestamp|kube_pod_container_status_restarts_total|kube_pod_container_status_waiting_reason|kube_pod_info|kube_pod_owner|kube_pod_restart_policy|kube_pod_spec_volumes_persistentvolumeclaims_info|kube_pod_start_time|kube_pod_status_phase|kube_pod_status_reason|kube_replicaset.*|kube_resourcequota|kube_secret_metadata_resource_version|kube_statefulset.*"
          action = "keep"
        }
        forward_to = argument.metrics_destinations.value
      }
    
      // Node Exporter
      discovery.kubernetes "node_exporter" {
        role = "pod"
    
        selectors {
          role = "pod"
          label = "app.kubernetes.io/name=node-exporter,release=grafana-k8s-monitoring"
        }
        namespaces {
          names = ["default"]
        }
      }
    
      discovery.relabel "node_exporter" {
        targets = discovery.kubernetes.node_exporter.targets
    
        // keep only the specified metrics port name, and pods that are Running and ready
        rule {
          source_labels = [
            "__meta_kubernetes_pod_container_port_name",
            "__meta_kubernetes_pod_container_init",
            "__meta_kubernetes_pod_phase",
            "__meta_kubernetes_pod_ready",
          ]
          separator = "@"
          regex = "metrics@false@Running@true"
          action = "keep"
        }
    
        // Set the instance label to the node name
        rule {
          source_labels = ["__meta_kubernetes_pod_node_name"]
          action = "replace"
          target_label = "instance"
        }
    
        // set the namespace label
        rule {
          source_labels = ["__meta_kubernetes_namespace"]
          target_label  = "namespace"
        }
    
        // set the pod label
        rule {
          source_labels = ["__meta_kubernetes_pod_name"]
          target_label  = "pod"
        }
    
        // set the container label
        rule {
          source_labels = ["__meta_kubernetes_pod_container_name"]
          target_label  = "container"
        }
    
        // set a workload label
        rule {
          source_labels = [
            "__meta_kubernetes_pod_controller_kind",
            "__meta_kubernetes_pod_controller_name",
          ]
          separator = "/"
          target_label  = "workload"
        }
        // remove the hash from the ReplicaSet
        rule {
          source_labels = ["workload"]
          regex = "(ReplicaSet/.+)-.+"
          target_label  = "workload"
        }
    
        // set the app name if specified as metadata labels "app:" or "app.kubernetes.io/name:" or "k8s-app:"
        rule {
          action = "replace"
          source_labels = [
            "__meta_kubernetes_pod_label_app_kubernetes_io_name",
            "__meta_kubernetes_pod_label_k8s_app",
            "__meta_kubernetes_pod_label_app",
          ]
          separator = ";"
          regex = "^(?:;*)?([^;]+).*$"
          replacement = "$1"
          target_label = "app"
        }
    
        // set the component if specified as metadata labels "component:" or "app.kubernetes.io/component:" or "k8s-component:"
        rule {
          action = "replace"
          source_labels = [
            "__meta_kubernetes_pod_label_app_kubernetes_io_component",
            "__meta_kubernetes_pod_label_k8s_component",
            "__meta_kubernetes_pod_label_component",
          ]
          regex = "^(?:;*)?([^;]+).*$"
          replacement = "$1"
          target_label = "component"
        }
    
        // set a source label
        rule {
          action = "replace"
          replacement = "kubernetes"
          target_label = "source"
        }
      }
    
      prometheus.scrape "node_exporter" {
        targets = discovery.relabel.node_exporter.output
        job_name = "integrations/node_exporter"
        scrape_interval = "60s"
        scrape_protocols = ["OpenMetricsText1.0.0","OpenMetricsText0.0.1","PrometheusText0.0.4"]
        scrape_classic_histograms = false
        scheme = "http"
        bearer_token_file = ""
        tls_config {
          insecure_skip_verify = true
        }
    
        clustering {
          enabled = true
        }
        forward_to = [prometheus.relabel.node_exporter.receiver]
      }
    
      prometheus.relabel "node_exporter" {
        max_cache_size = 100000
        rule {
          source_labels = ["__name__"]
          regex = "up|scrape_samples_scraped|node_cpu.*|node_exporter_build_info|node_filesystem.*|node_memory.*|node_network_receive_bytes_total|node_network_receive_drop_total|node_network_transmit_bytes_total|node_network_transmit_drop_total|process_cpu_seconds_total|process_resident_memory_bytes"
          action = "keep"
        }
        // Drop metrics for certain file systems
        rule {
          source_labels = ["__name__", "fstype"]
          separator = "@"
          regex = "node_filesystem.*@(ramfs|tmpfs)"
          action = "drop"
        }
        forward_to = argument.metrics_destinations.value
      }
    
      // Windows Exporter
      discovery.kubernetes "windows_exporter_pods" {
        role = "pod"
        selectors {
          role = "pod"
          label = "app.kubernetes.io/name=windows-exporter,release=grafana-k8s-monitoring"
        }
        namespaces {
          names = ["default"]
        }
      }
    
      discovery.relabel "windows_exporter" {
        targets = discovery.kubernetes.windows_exporter_pods.targets
    
        // keep only the specified metrics port name, and pods that are Running and ready
        rule {
          source_labels = [
            "__meta_kubernetes_pod_container_port_name",
            "__meta_kubernetes_pod_container_init",
            "__meta_kubernetes_pod_phase",
            "__meta_kubernetes_pod_ready",
          ]
          separator = "@"
          regex = "metrics@false@Running@true"
          action = "keep"
        }
    
        // Set the instance label to the node name
        rule {
          source_labels = ["__meta_kubernetes_pod_node_name"]
          target_label = "instance"
        }
      }
    
      prometheus.scrape "windows_exporter" {
        targets  = discovery.relabel.windows_exporter.output
        job_name   = "integrations/windows-exporter"
        scrape_interval = "60s"
        scrape_protocols = ["OpenMetricsText1.0.0","OpenMetricsText0.0.1","PrometheusText0.0.4"]
        scrape_classic_histograms = false
        clustering {
          enabled = true
        }
        forward_to = [prometheus.relabel.windows_exporter.receiver]
      }
    
      prometheus.relabel "windows_exporter" {
        max_cache_size = 100000
        rule {
          source_labels = ["__name__"]
          regex = "up|scrape_samples_scraped|windows_.*|node_cpu_seconds_total|node_filesystem_size_bytes|node_filesystem_avail_bytes|container_cpu_usage_seconds_total"
          action = "keep"
        }
        forward_to = argument.metrics_destinations.value
      }
    
      discovery.kubernetes "kepler" {
        role = "pod"
        namespaces {
          own_namespace = true
        }
        selectors {
          role = "pod"
          label = "app.kubernetes.io/name=kepler"
        }
      }
    
      discovery.relabel "kepler" {
        targets = discovery.kubernetes.kepler.targets
        rule {
          source_labels = ["__meta_kubernetes_pod_node_name"]
          action = "replace"
          target_label = "instance"
        }
      }
    
      prometheus.scrape "kepler" {
        targets      = discovery.relabel.kepler.output
        job_name     = "integrations/kepler"
        honor_labels = true
        scrape_interval = "60s"
        scrape_protocols = ["OpenMetricsText1.0.0","OpenMetricsText0.0.1","PrometheusText0.0.4"]
        scrape_classic_histograms = false
        clustering {
          enabled = true
        }
        forward_to = [prometheus.relabel.kepler.receiver]
      }
    
      prometheus.relabel "kepler" {
        max_cache_size = 100000
        rule {
          source_labels = ["__name__"]
          regex = "up|scrape_samples_scraped|kepler_.*"
          action = "keep"
        }
        forward_to = argument.metrics_destinations.value
      }
    
      discovery.kubernetes "opencost" {
        role = "pod"
        namespaces {
          own_namespace = true
        }
        selectors {
          role = "pod"
          label = "app.kubernetes.io/name=opencost"
        }
      }
    
      discovery.relabel "opencost" {
        targets = discovery.kubernetes.opencost.targets
        rule {
          source_labels = ["__meta_kubernetes_pod_node_name"]
          action = "replace"
          target_label = "instance"
        }
      }
    
      prometheus.scrape "opencost" {
        targets      = discovery.relabel.opencost.output
        job_name     = "integrations/opencost"
        honor_labels = true
        scrape_interval = "60s"
        scrape_protocols = ["OpenMetricsText1.0.0","OpenMetricsText0.0.1","PrometheusText0.0.4"]
        scrape_classic_histograms = false
        clustering {
          enabled = true
        }
        forward_to = [prometheus.relabel.opencost.receiver]
      }
    
      prometheus.relabel "opencost" {
        max_cache_size = 100000
        rule {
          source_labels = ["__name__"]
          regex = "up|scrape_samples_scraped|container_cpu_allocation|container_gpu_allocation|container_memory_allocation_bytes|deployment_match_labels|kubecost_cluster_info|kubecost_cluster_management_cost|kubecost_cluster_memory_working_set_bytes|kubecost_http_requests_total|kubecost_http_response_size_bytes|kubecost_http_response_time_seconds|kubecost_load_balancer_cost|kubecost_network_internet_egress_cost|kubecost_network_region_egress_cost|kubecost_network_zone_egress_cost|kubecost_node_is_spot|node_cpu_hourly_cost|node_gpu_count|node_gpu_hourly_cost|node_ram_hourly_cost|node_total_hourly_cost|opencost_build_info|pod_pvc_allocation|pv_hourly_cost|service_selector_labels|statefulSet_match_labels"
          action = "keep"
        }
        forward_to = argument.metrics_destinations.value
      }
    }
    cluster_metrics "feature" {
      metrics_destinations = [
        prometheus.remote_write.grafana_cloud_metrics.receiver,
      ]
    }
    // Feature: Prometheus Operator Objects
    declare "prometheus_operator_objects" {
      argument "metrics_destinations" {
        comment = "Must be a list of metric destinations where collected metrics should be forwarded to"
      }
    
      // Prometheus Operator PodMonitor objects
      prometheus.operator.podmonitors "pod_monitors" {
        clustering {
          enabled = true
        }
        scrape {
          default_scrape_interval = "60s"
        }
        forward_to = argument.metrics_destinations.value
      }
    
      // Prometheus Operator Probe objects
      prometheus.operator.probes "probes" {
        clustering {
          enabled = true
        }
        scrape {
          default_scrape_interval = "60s"
        }
        forward_to = argument.metrics_destinations.value
      }
    
      // Prometheus Operator ServiceMonitor objects
      prometheus.operator.servicemonitors "service_monitors" {
        clustering {
          enabled = true
        }
        scrape {
          default_scrape_interval = "60s"
        }
        forward_to = argument.metrics_destinations.value
      }
    }
    prometheus_operator_objects "feature" {
      metrics_destinations = [
        prometheus.remote_write.grafana_cloud_metrics.receiver,
      ]
    }
    
    
    remote.kubernetes.secret "alloy_metrics_remote_cfg" {
      name      = "alloy-metrics-remote-cfg-grafana-k8s-monitoring"
      namespace = "default"
    }
    
    remotecfg {
      id = sys.env("GCLOUD_FM_COLLECTOR_ID")
      url = "https://fleet-management-prod-011.grafana.net"
      basic_auth {
        username = convert.nonsensitive(remote.kubernetes.secret.alloy_metrics_remote_cfg.data["username"])
        password = remote.kubernetes.secret.alloy_metrics_remote_cfg.data["password"]
      }
      tls_config {
        insecure_skip_verify = false
      }
      poll_frequency = "5m"
      attributes = {
        "cluster" = "my-cluster",
        "namespace" = "default",
        "platform" = "kubernetes",
        "release" = "grafana-k8s-monitoring",
        "source" = "k8s-monitoring",
        "sourceVersion" = "3.4.2",
        "workloadName" = "alloy-metrics",
        "workloadType" = "statefulset",
      }
    }
    
    // Destination: grafana-cloud-metrics (prometheus)
    otelcol.exporter.prometheus "grafana_cloud_metrics" {
      add_metric_suffixes = true
      resource_to_telemetry_conversion = false
      forward_to = [prometheus.remote_write.grafana_cloud_metrics.receiver]
    }
    
    prometheus.remote_write "grafana_cloud_metrics" {
      endpoint {
        url = "https://prometheus-prod-24-prod-eu-west-2.grafana.net./api/prom/push"
        headers = {
        }
        basic_auth {
          username = convert.nonsensitive(remote.kubernetes.secret.grafana_cloud_metrics.data["username"])
          password = remote.kubernetes.secret.grafana_cloud_metrics.data["password"]
        }
        tls_config {
          insecure_skip_verify = false
        }
        send_native_histograms = false
    
        queue_config {
          capacity = 10000
          min_shards = 1
          max_shards = 50
          max_samples_per_send = 2000
          batch_send_deadline = "5s"
          min_backoff = "30ms"
          max_backoff = "5s"
          retry_on_http_429 = true
          sample_age_limit = "0s"
        }
    
        write_relabel_config {
          source_labels = ["cluster"]
          regex = ""
          replacement = "my-cluster"
          target_label = "cluster"
        }
        write_relabel_config {
          source_labels = ["k8s_cluster_name"]
          regex = ""
          replacement = "my-cluster"
          target_label = "k8s_cluster_name"
        }
      }
    
      wal {
        truncate_frequency = "2h"
        min_keepalive_time = "5m"
        max_keepalive_time = "8h"
      }
    }
    
    remote.kubernetes.secret "grafana_cloud_metrics" {
      name      = "grafana-cloud-metrics-grafana-k8s-monitoring"
      namespace = "default"
    }
---
# Source: k8s-monitoring/templates/alloy-config.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: grafana-k8s-monitoring-alloy-singleton
  namespace: grafana
data:
  config.alloy: |
    // Feature: Cluster Events
    declare "cluster_events" {
      argument "logs_destinations" {
        comment = "Must be a list of log destinations where collected logs should be forwarded to"
      }
    
      loki.source.kubernetes_events "cluster_events" {
        job_name   = "integrations/kubernetes/eventhandler"
        log_format = "logfmt"
        forward_to = [loki.process.cluster_events.receiver]
      }
    
      loki.process "cluster_events" {
    
        // add a static source label to the logs so they can be differentiated / restricted if necessary
        stage.static_labels {
          values = {
            "source" = "kubernetes-events",
          }
        }
    
        // extract some of the fields from the log line, these could be used as labels, structured metadata, etc.
        stage.logfmt {
          mapping = {
            "component" = "sourcecomponent", // map the sourcecomponent field to component
            "kind" = "",
            "level" = "type", // most events don't have a level but they do have a "type" i.e. Normal, Warning, Error, etc.
            "name" = "",
            "node" = "sourcehost", // map the sourcehost field to node
            "reason" = "",
          }
        }
        // set these values as labels, they may or may not be used as index labels in Loki as they can be dropped
        // prior to being written to Loki, but this makes them available
        stage.labels {
          values = {
            "component" = "",
            "kind" = "",
            "level" = "",
            "name" = "",
            "node" = "",
            "reason" = "",
          }
        }
    
        // if kind=Node, set the node label by copying the name field
        stage.match {
          selector = "{kind=\"Node\"}"
    
          stage.labels {
            values = {
              "node" = "name",
            }
          }
        }
    
        // set the level extracted key value as a normalized log level
        stage.match {
          selector = "{level=\"Normal\"}"
    
          stage.static_labels {
            values = {
              level = "Info",
            }
          }
        }
        stage.structured_metadata {
          values = {
            "name" = "name",
          }
        }
    
        // Only keep the labels that are defined in the `keepLabels` list.
        stage.label_keep {
          values = ["job","level","namespace","node","source","reason"]
        }
        stage.labels {
          values = {
            "service_name" = "job",
          }
        }
        forward_to = argument.logs_destinations.value
      }
    }
    cluster_events "feature" {
      logs_destinations = [
        loki.write.grafana_cloud_logs.receiver,
      ]
    }
    // Self Reporting
    prometheus.exporter.unix "kubernetes_monitoring_telemetry" {
      set_collectors = ["textfile"]
      textfile {
        directory = "/etc/alloy"
      }
    }
    
    discovery.relabel "kubernetes_monitoring_telemetry" {
      targets = prometheus.exporter.unix.kubernetes_monitoring_telemetry.targets
      rule {
        target_label = "instance"
        action = "replace"
        replacement = "grafana-k8s-monitoring"
      }
      rule {
        target_label = "job"
        action = "replace"
        replacement = "integrations/kubernetes/kubernetes_monitoring_telemetry"
      }
    }
    
    prometheus.scrape "kubernetes_monitoring_telemetry" {
      job_name   = "integrations/kubernetes/kubernetes_monitoring_telemetry"
      targets    = discovery.relabel.kubernetes_monitoring_telemetry.output
      scrape_interval = "60s"
      clustering {
        enabled = true
      }
      forward_to = [prometheus.relabel.kubernetes_monitoring_telemetry.receiver]
    }
    
    prometheus.relabel "kubernetes_monitoring_telemetry" {
      rule {
        source_labels = ["__name__"]
        regex = "grafana_kubernetes_monitoring_.*"
        action = "keep"
      }
      forward_to = [
        prometheus.remote_write.grafana_cloud_metrics.receiver,
      ]
    }
    
    
    remote.kubernetes.secret "alloy_singleton_remote_cfg" {
      name      = "alloy-singleton-remote-cfg-grafana-k8s-monitoring"
      namespace = "default"
    }
    
    remotecfg {
      id = sys.env("GCLOUD_FM_COLLECTOR_ID")
      url = "https://fleet-management-prod-011.grafana.net"
      basic_auth {
        username = convert.nonsensitive(remote.kubernetes.secret.alloy_singleton_remote_cfg.data["username"])
        password = remote.kubernetes.secret.alloy_singleton_remote_cfg.data["password"]
      }
      tls_config {
        insecure_skip_verify = false
      }
      poll_frequency = "5m"
      attributes = {
        "cluster" = "my-cluster",
        "namespace" = "default",
        "platform" = "kubernetes",
        "release" = "grafana-k8s-monitoring",
        "source" = "k8s-monitoring",
        "sourceVersion" = "3.4.2",
        "workloadName" = "alloy-singleton",
        "workloadType" = "deployment",
      }
    }
    
    // Destination: grafana-cloud-metrics (prometheus)
    otelcol.exporter.prometheus "grafana_cloud_metrics" {
      add_metric_suffixes = true
      resource_to_telemetry_conversion = false
      forward_to = [prometheus.remote_write.grafana_cloud_metrics.receiver]
    }
    
    prometheus.remote_write "grafana_cloud_metrics" {
      endpoint {
        url = "https://prometheus-prod-24-prod-eu-west-2.grafana.net./api/prom/push"
        headers = {
        }
        basic_auth {
          username = convert.nonsensitive(remote.kubernetes.secret.grafana_cloud_metrics.data["username"])
          password = remote.kubernetes.secret.grafana_cloud_metrics.data["password"]
        }
        tls_config {
          insecure_skip_verify = false
        }
        send_native_histograms = false
    
        queue_config {
          capacity = 10000
          min_shards = 1
          max_shards = 50
          max_samples_per_send = 2000
          batch_send_deadline = "5s"
          min_backoff = "30ms"
          max_backoff = "5s"
          retry_on_http_429 = true
          sample_age_limit = "0s"
        }
    
        write_relabel_config {
          source_labels = ["cluster"]
          regex = ""
          replacement = "my-cluster"
          target_label = "cluster"
        }
        write_relabel_config {
          source_labels = ["k8s_cluster_name"]
          regex = ""
          replacement = "my-cluster"
          target_label = "k8s_cluster_name"
        }
      }
    
      wal {
        truncate_frequency = "2h"
        min_keepalive_time = "5m"
        max_keepalive_time = "8h"
      }
    }
    
    remote.kubernetes.secret "grafana_cloud_metrics" {
      name      = "grafana-cloud-metrics-grafana-k8s-monitoring"
      namespace = "default"
    }
    
    // Destination: grafana-cloud-logs (loki)
    otelcol.exporter.loki "grafana_cloud_logs" {
      forward_to = [loki.write.grafana_cloud_logs.receiver]
    }
    
    loki.write "grafana_cloud_logs" {
      endpoint {
        url = "https://logs-prod-012.grafana.net./loki/api/v1/push"
        basic_auth {
          username = convert.nonsensitive(remote.kubernetes.secret.grafana_cloud_logs.data["username"])
          password = remote.kubernetes.secret.grafana_cloud_logs.data["password"]
        }
        tls_config {
          insecure_skip_verify = false
        }
        min_backoff_period = "500ms"
        max_backoff_period = "5m"
        max_backoff_retries = "10"
      }
      external_labels = {
        "cluster" = "my-cluster",
        "k8s_cluster_name" = "my-cluster",
      }
    }
    
    remote.kubernetes.secret "grafana_cloud_logs" {
      name      = "grafana-cloud-logs-grafana-k8s-monitoring"
      namespace = "default"
    }
  self-reporting-metric.prom: |
    # HELP grafana_kubernetes_monitoring_build_info A metric to report the version of the Kubernetes Monitoring Helm chart
    # TYPE grafana_kubernetes_monitoring_build_info gauge
    grafana_kubernetes_monitoring_build_info{version="3.4.2", namespace="default"} 1
    # HELP grafana_kubernetes_monitoring_feature_info A metric to report the enabled features of the Kubernetes Monitoring Helm chart
    # TYPE grafana_kubernetes_monitoring_feature_info gauge
    grafana_kubernetes_monitoring_feature_info{feature="annotationAutodiscovery", version="1.0.0"} 1
    grafana_kubernetes_monitoring_feature_info{feature="applicationObservability", protocols="otlpgrpc,otlphttp,zipkin", version="1.0.0"} 1
    grafana_kubernetes_monitoring_feature_info{feature="autoInstrumentation", version="1.0.0"} 1
    grafana_kubernetes_monitoring_feature_info{deployments="kube-state-metrics,node-exporter,windows-exporter,kepler", feature="clusterMetrics", sources="kubelet,kubeletResource,cadvisor,kube-state-metrics,node-exporter,windows-exporter,kepler", version="1.0.0"} 1
    grafana_kubernetes_monitoring_feature_info{feature="clusterEvents", version="1.0.0"} 1
    grafana_kubernetes_monitoring_feature_info{feature="nodeLogs", version="1.0.0"} 1
    grafana_kubernetes_monitoring_feature_info{feature="podLogs", method="volumes", version="1.0.0"} 1
    grafana_kubernetes_monitoring_feature_info{feature="profiling", method="", version="1.0.0"} 1
    grafana_kubernetes_monitoring_feature_info{feature="prometheusOperatorObjects", sources="ServiceMonitors,PodMonitors,Probes", version="1.0.0"} 1
    # EOF
---
# Source: k8s-monitoring/templates/alloy-config.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: grafana-k8s-monitoring-alloy-logs
  namespace: grafana
data:
  config.alloy: |
    // Feature: Node Logs
    declare "node_logs" {
      argument "logs_destinations" {
        comment = "Must be a list of log destinations where collected logs should be forwarded to"
      }
    
      loki.relabel "journal" {
    
        // copy all journal labels and make the available to the pipeline stages as labels, there is a label
        // keep defined to filter out unwanted labels, these pipeline labels can be set as structured metadata
        // as well, the following labels are available:
        // - boot_id
        // - cap_effective
        // - cmdline
        // - comm
        // - exe
        // - gid
        // - hostname
        // - machine_id
        // - pid
        // - stream_id
        // - systemd_cgroup
        // - systemd_invocation_id
        // - systemd_slice
        // - systemd_unit
        // - transport
        // - uid
        //
        // More Info: https://www.freedesktop.org/software/systemd/man/systemd.journal-fields.html
        rule {
          action = "labelmap"
          regex = "__journal__(.+)"
        }
    
        rule {
          action = "replace"
          source_labels = ["__journal__systemd_unit"]
          replacement = "$1"
          target_label = "unit"
        }
    
        // the service_name label will be set automatically in loki if not set, and the unit label
        // will not allow service_name to be set automatically.
        rule {
          action = "replace"
          source_labels = ["__journal__systemd_unit"]
          replacement = "$1"
          target_label = "service_name"
        }
    
        forward_to = [] // No forward_to is used in this component, the defined rules are used in the loki.source.journal component
      }
    
      loki.source.journal "worker" {
        path = "/var/log/journal"
        format_as_json = false
        max_age = "8h"
        relabel_rules = loki.relabel.journal.rules
        labels = {
          job = "integrations/kubernetes/journal",
          instance = sys.env("HOSTNAME"),
        }
        forward_to = [loki.process.journal_logs.receiver]
      }
    
      loki.process "journal_logs" {
        stage.static_labels {
          values = {
            // add a static source label to the logs so they can be differentiated / restricted if necessary
            "source" = "journal",
            // default level to unknown
            level = "unknown",
          }
        }
    
        // Attempt to determine the log level, most k8s workers are either in logfmt or klog formats
        // check to see if the log line matches the klog format (https://github.com/kubernetes/klog)
        stage.match {
          // unescaped regex: ([IWED][0-9]{4}\s+[0-9]{2}:[0-9]{2}:[0-9]{2}\.[0-9]+)
          selector = "{level=\"unknown\"} |~ \"([IWED][0-9]{4}\\\\s+[0-9]{2}:[0-9]{2}:[0-9]{2}\\\\.[0-9]+)\""
    
          // extract log level, klog uses a single letter code for the level followed by the month and day i.e. I0119
          stage.regex {
            expression = "((?P<level>[A-Z])[0-9])"
          }
    
          // if the extracted level is I set INFO
          stage.replace {
            source = "level"
            expression = "(I)"
            replace = "INFO"
          }
    
          // if the extracted level is W set WARN
          stage.replace {
            source = "level"
            expression = "(W)"
            replace = "WARN"
          }
    
          // if the extracted level is E set ERROR
          stage.replace {
            source = "level"
            expression = "(E)"
            replace = "ERROR"
          }
    
          // if the extracted level is I set INFO
          stage.replace {
            source = "level"
            expression = "(D)"
            replace = "DEBUG"
          }
    
          // set the extracted level to be a label
          stage.labels {
            values = {
              level = "",
            }
          }
        }
    
        // if the level is still unknown, do one last attempt at detecting it based on common levels
        stage.match {
          selector = "{level=\"unknown\"}"
    
          // unescaped regex: (?i)(?:"(?:level|loglevel|levelname|lvl|levelText|SeverityText)":\s*"|\s*(?:level|loglevel|levelText|lvl)="?|\s+\[?)(?P<level>(DEBUG?|DBG|INFO?(RMATION)?|WA?RN(ING)?|ERR(OR)?|CRI?T(ICAL)?|FATAL|FTL|NOTICE|TRACE|TRC|PANIC|PNC|ALERT|EMERGENCY))("|\s+|-|\s*\])
          stage.regex {
            expression = "(?i)(?:\"(?:level|loglevel|levelname|lvl|levelText|SeverityText)\":\\s*\"|\\s*(?:level|loglevel|levelText|lvl)=\"?|\\s+\\[?)(?P<level>(DEBUG?|DBG|INFO?(RMATION)?|WA?RN(ING)?|ERR(OR)?|CRI?T(ICAL)?|FATAL|FTL|NOTICE|TRACE|TRC|PANIC|PNC|ALERT|EMERGENCY))(\"|\\s+|-|\\s*\\])"
          }
    
          // set the extracted level to be a label
          stage.labels {
            values = {
              level = "",
            }
          }
        }
    
        // Only keep the labels that are defined in the `keepLabels` list.
        stage.label_keep {
          values = ["__tenant_id__","instance","job","level","name","unit","service_name","source"]
        }
    
        forward_to = argument.logs_destinations.value
      }
    }
    node_logs "feature" {
      logs_destinations = [
        loki.write.grafana_cloud_logs.receiver,
      ]
    }
    // Feature: Pod Logs
    declare "pod_logs" {
      argument "logs_destinations" {
        comment = "Must be a list of log destinations where collected logs should be forwarded to"
      }
    
      discovery.relabel "filtered_pods" {
        targets = discovery.kubernetes.pods.targets
        rule {
          source_labels = ["__meta_kubernetes_namespace"]
          action = "replace"
          target_label = "namespace"
        }
        rule {
          source_labels = ["__meta_kubernetes_pod_name"]
          action = "replace"
          target_label = "pod"
        }
        rule {
          source_labels = ["__meta_kubernetes_pod_container_name"]
          action = "replace"
          target_label = "container"
        }
        rule {
          source_labels = ["__meta_kubernetes_namespace", "__meta_kubernetes_pod_container_name"]
          separator = "/"
          action = "replace"
          replacement = "$1"
          target_label = "job"
        }
    
        // set the container runtime as a label
        rule {
          action = "replace"
          source_labels = ["__meta_kubernetes_pod_container_id"]
          regex = "^(\\S+):\\/\\/.+$"
          replacement = "$1"
          target_label = "tmp_container_runtime"
        }
    
        // make all labels on the pod available to the pipeline as labels,
        // they are omitted before write to loki via stage.label_keep unless explicitly set
        rule {
          action = "labelmap"
          regex = "__meta_kubernetes_pod_label_(.+)"
        }
    
        // make all annotations on the pod available to the pipeline as labels,
        // they are omitted before write to loki via stage.label_keep unless explicitly set
        rule {
          action = "labelmap"
          regex = "__meta_kubernetes_pod_annotation_(.+)"
        }
    
        // explicitly set service_name. if not set, loki will automatically try to populate a default.
        // see https://grafana.com/docs/loki/latest/get-started/labels/#default-labels-for-all-users
        //
        // choose the first value found from the following ordered list:
        // - pod.annotation[resource.opentelemetry.io/service.name]
        // - pod.label[app.kubernetes.io/name]
        // - k8s.pod.name
        // - k8s.container.name
        rule {
          action = "replace"
          source_labels = [
            "__meta_kubernetes_pod_annotation_resource_opentelemetry_io_service_name",
            "__meta_kubernetes_pod_label_app_kubernetes_io_name",
            "__meta_kubernetes_pod_container_name",
          ]
          separator = ";"
          regex = "^(?:;*)?([^;]+).*$"
          replacement = "$1"
          target_label = "service_name"
        }
    
        // explicitly set service_namespace.
        //
        // choose the first value found from the following ordered list:
        // - pod.annotation[resource.opentelemetry.io/service.namespace]
        // - pod.namespace
        rule {
          action = "replace"
          source_labels = [
            "__meta_kubernetes_pod_annotation_resource_opentelemetry_io_service_namespace",
            "namespace",
          ]
          separator = ";"
          regex = "^(?:;*)?([^;]+).*$"
          replacement = "$1"
          target_label = "service_namespace"
        }
    
        // explicitly set service_instance_id.
        //
        // choose the first value found from the following ordered list:
        // - pod.annotation[resource.opentelemetry.io/service.instance.id]
        // - concat([k8s.namespace.name, k8s.pod.name, k8s.container.name], '.')
        rule {
          source_labels = ["__meta_kubernetes_pod_annotation_resource_opentelemetry_io_service_instance_id"]
          target_label = "service_instance_id"
        }
        rule {
          source_labels = ["service_instance_id", "namespace", "pod", "container"]
          separator = "."
          regex = "^\\.([^.]+\\.[^.]+\\.[^.]+)$"
          target_label = "service_instance_id"
        }
    
        // set resource attributes
        rule {
          action = "labelmap"
          regex = "__meta_kubernetes_pod_annotation_resource_opentelemetry_io_(.+)"
        }
        rule {
          source_labels = ["__meta_kubernetes_pod_annotation_k8s_grafana_com_logs_job"]
          regex = "(.+)"
          target_label = "job"
        }
        rule {
          source_labels = ["__meta_kubernetes_pod_label_app_kubernetes_io_name"]
          regex = "(.+)"
          target_label = "app_kubernetes_io_name"
        }
      }
    
      discovery.kubernetes "pods" {
        role = "pod"
        selectors {
          role = "pod"
          field = "spec.nodeName=" + sys.env("HOSTNAME")
        }
      }
    
      discovery.relabel "filtered_pods_with_paths" {
        targets = discovery.relabel.filtered_pods.output
    
        rule {
          source_labels = ["__meta_kubernetes_pod_uid", "__meta_kubernetes_pod_container_name"]
          separator = "/"
          action = "replace"
          replacement = "/var/log/pods/*$1/*.log"
          target_label = "__path__"
        }
      }
    
      local.file_match "pod_logs" {
        path_targets = discovery.relabel.filtered_pods_with_paths.output
      }
    
      loki.source.file "pod_logs" {
        targets    = local.file_match.pod_logs.targets
        forward_to = [loki.process.pod_logs.receiver]
      }
    
      loki.process "pod_logs" {
        stage.match {
          selector = "{tmp_container_runtime=~\"containerd|cri-o\"}"
          // the cri processing stage extracts the following k/v pairs: log, stream, time, flags
          stage.cri {}
    
          // Set the extract flags and stream values as labels
          stage.labels {
            values = {
              flags  = "",
              stream  = "",
            }
          }
        }
    
        stage.match {
          selector = "{tmp_container_runtime=\"docker\"}"
          // the docker processing stage extracts the following k/v pairs: log, stream, time
          stage.docker {}
    
          // Set the extract stream value as a label
          stage.labels {
            values = {
              stream  = "",
            }
          }
        }
    
        // Drop the filename label, since it's not really useful in the context of Kubernetes, where we already have cluster,
        // namespace, pod, and container labels. Drop any structured metadata. Also drop the temporary
        // container runtime label as it is no longer needed.
        stage.label_drop {
          values = [
            "filename",
            "tmp_container_runtime",
          ]
        }
        stage.structured_metadata {
          values = {
            "k8s_pod_name" = "k8s_pod_name",
            "pod" = "pod",
            "service_instance_id" = "service_instance_id",
          }
        }
    
        // Only keep the labels that are defined in the `keepLabels` list.
        stage.label_keep {
          values = ["__tenant_id__","app_kubernetes_io_name","container","instance","job","level","namespace","service_name","service_namespace","deployment_environment","deployment_environment_name","k8s_namespace_name","k8s_deployment_name","k8s_statefulset_name","k8s_daemonset_name","k8s_cronjob_name","k8s_job_name","k8s_node_name"]
        }
    
        forward_to = argument.logs_destinations.value
      }
    }
    pod_logs "feature" {
      logs_destinations = [
        loki.write.grafana_cloud_logs.receiver,
      ]
    }
    
    
    remote.kubernetes.secret "alloy_logs_remote_cfg" {
      name      = "alloy-logs-remote-cfg-grafana-k8s-monitoring"
      namespace = "default"
    }
    
    remotecfg {
      id = sys.env("GCLOUD_FM_COLLECTOR_ID")
      url = "https://fleet-management-prod-011.grafana.net"
      basic_auth {
        username = convert.nonsensitive(remote.kubernetes.secret.alloy_logs_remote_cfg.data["username"])
        password = remote.kubernetes.secret.alloy_logs_remote_cfg.data["password"]
      }
      tls_config {
        insecure_skip_verify = false
      }
      poll_frequency = "5m"
      attributes = {
        "cluster" = "my-cluster",
        "namespace" = "default",
        "platform" = "kubernetes",
        "release" = "grafana-k8s-monitoring",
        "source" = "k8s-monitoring",
        "sourceVersion" = "3.4.2",
        "workloadName" = "alloy-logs",
        "workloadType" = "daemonset",
      }
    }
    
    // Destination: grafana-cloud-logs (loki)
    otelcol.exporter.loki "grafana_cloud_logs" {
      forward_to = [loki.write.grafana_cloud_logs.receiver]
    }
    
    loki.write "grafana_cloud_logs" {
      endpoint {
        url = "https://logs-prod-012.grafana.net./loki/api/v1/push"
        basic_auth {
          username = convert.nonsensitive(remote.kubernetes.secret.grafana_cloud_logs.data["username"])
          password = remote.kubernetes.secret.grafana_cloud_logs.data["password"]
        }
        tls_config {
          insecure_skip_verify = false
        }
        min_backoff_period = "500ms"
        max_backoff_period = "5m"
        max_backoff_retries = "10"
      }
      external_labels = {
        "cluster" = "my-cluster",
        "k8s_cluster_name" = "my-cluster",
      }
    }
    
    remote.kubernetes.secret "grafana_cloud_logs" {
      name      = "grafana-cloud-logs-grafana-k8s-monitoring"
      namespace = "default"
    }
---
# Source: k8s-monitoring/templates/alloy-config.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: grafana-k8s-monitoring-alloy-receiver
  namespace: grafana
data:
  config.alloy: |
    // Feature: Application Observability
    declare "application_observability" {
      argument "metrics_destinations" {
        comment = "Must be a list of metrics destinations where collected metrics should be forwarded to"
      }
    
      argument "logs_destinations" {
        comment = "Must be a list of log destinations where collected logs should be forwarded to"
      }
    
      argument "traces_destinations" {
        comment = "Must be a list of trace destinations where collected trace should be forwarded to"
      }
    
      // OTLP Receiver
      otelcol.receiver.otlp "receiver" {
        grpc {
          endpoint = "0.0.0.0:4317"
          include_metadata = false
          max_recv_msg_size = "4MiB"
          read_buffer_size = "512KiB"
          write_buffer_size = "32KiB"
        }
        http {
          endpoint = "0.0.0.0:4318"
          include_metadata = false
          max_request_body_size = "20MiB"
        }
        debug_metrics {
          disable_high_cardinality_metrics = true
        }
        output {
          metrics = [otelcol.processor.resourcedetection.default.input]
          logs = [otelcol.processor.resourcedetection.default.input]
          traces = [otelcol.processor.resourcedetection.default.input]
        }
      }
    
      // Zipkin Receiver
      otelcol.receiver.zipkin "receiver" {
        endpoint = "0.0.0.0:9411"
        debug_metrics {
          disable_high_cardinality_metrics = true
        }
        output {
          traces = [otelcol.processor.resourcedetection.default.input]
        }
      }
    
      // Resource Detection Processor
      otelcol.processor.resourcedetection "default" {
        detectors = ["env","system"]
        override = true
    
        system {
          hostname_sources = ["os"]
        }
    
        output {
          metrics = [otelcol.processor.k8sattributes.default.input]
          logs = [otelcol.processor.k8sattributes.default.input]
          traces = [otelcol.processor.k8sattributes.default.input]
        }
      }
    
      // K8s Attributes Processor
      otelcol.processor.k8sattributes "default" {
        passthrough = false
        extract {
          metadata = ["k8s.namespace.name","k8s.pod.name","k8s.deployment.name","k8s.statefulset.name","k8s.daemonset.name","k8s.cronjob.name","k8s.job.name","k8s.node.name","k8s.pod.uid","k8s.pod.start_time"]
        }
        pod_association {
          source {
            from = "resource_attribute"
            name = "k8s.pod.ip"
          }
        }
        pod_association {
          source {
            from = "resource_attribute"
            name = "k8s.pod.uid"
          }
        }
        pod_association {
          source {
            from = "connection"
          }
        }
    
        output {
          metrics = [otelcol.processor.transform.default.input]
          logs = [otelcol.processor.transform.default.input]
          traces = [otelcol.processor.transform.default.input, otelcol.connector.host_info.default.input]
        }
      }
    
      // Host Info Connector
      otelcol.connector.host_info "default" {
        host_identifiers = [ "k8s.node.name" ]
    
        output {
          metrics = [otelcol.processor.batch.default.input]
        }
      }
    
      // Transform Processor
      otelcol.processor.transform "default" {
        error_mode = "ignore"
        log_statements {
          context = "resource"
          statements = [
            "set(attributes[\"pod\"], attributes[\"k8s.pod.name\"])",
            "set(attributes[\"namespace\"], attributes[\"k8s.namespace.name\"])",
            "set(attributes[\"loki.resource.labels\"], \"cluster, namespace, job, pod\")",
          ]
        }
    
        output {
          metrics = [otelcol.processor.batch.default.input]
          logs = [otelcol.processor.batch.default.input]
          traces = [otelcol.processor.batch.default.input]
        }
      }
    
      // Batch Processor
      otelcol.processor.batch "default" {
        send_batch_size = 8192
        send_batch_max_size = 0
        timeout = "2s"
    
        output {
          metrics = argument.metrics_destinations.value
          logs = argument.logs_destinations.value
          traces = argument.traces_destinations.value
        }
      }
    }
    application_observability "feature" {
      metrics_destinations = [
        otelcol.processor.attributes.gc_otlp_endpoint.input,
      ]
      logs_destinations = [
        otelcol.processor.attributes.gc_otlp_endpoint.input,
      ]
      traces_destinations = [
        otelcol.processor.attributes.gc_otlp_endpoint.input,
      ]
    }
    
    
    remote.kubernetes.secret "alloy_receiver_remote_cfg" {
      name      = "alloy-receiver-remote-cfg-grafana-k8s-monitoring"
      namespace = "default"
    }
    
    remotecfg {
      id = sys.env("GCLOUD_FM_COLLECTOR_ID")
      url = "https://fleet-management-prod-011.grafana.net"
      basic_auth {
        username = convert.nonsensitive(remote.kubernetes.secret.alloy_receiver_remote_cfg.data["username"])
        password = remote.kubernetes.secret.alloy_receiver_remote_cfg.data["password"]
      }
      tls_config {
        insecure_skip_verify = false
      }
      poll_frequency = "5m"
      attributes = {
        "cluster" = "my-cluster",
        "namespace" = "default",
        "platform" = "kubernetes",
        "release" = "grafana-k8s-monitoring",
        "source" = "k8s-monitoring",
        "sourceVersion" = "3.4.2",
        "workloadName" = "alloy-receiver",
        "workloadType" = "daemonset",
      }
    }
    
    // Destination: gc-otlp-endpoint (otlp)
    otelcol.receiver.prometheus "gc_otlp_endpoint" {
      output {
        metrics = [otelcol.processor.attributes.gc_otlp_endpoint.input]
      }
    }
    otelcol.receiver.loki "gc_otlp_endpoint" {
      output {
        logs = [otelcol.processor.attributes.gc_otlp_endpoint.input]
      }
    }
    
    otelcol.processor.attributes "gc_otlp_endpoint" {
      output {
        metrics = [otelcol.processor.transform.gc_otlp_endpoint.input]
        logs = [otelcol.processor.transform.gc_otlp_endpoint.input]
        traces = [otelcol.processor.transform.gc_otlp_endpoint.input]
      }
    }
    
    otelcol.processor.transform "gc_otlp_endpoint" {
      error_mode = "ignore"
      metric_statements {
        context = "resource"
        statements = [
          `set(attributes["cluster"], "my-cluster")`,
          `set(attributes["k8s.cluster.name"], "my-cluster")`,
          `delete_key(attributes, "process.pid")`,
          `delete_key(attributes, "process.parent_pid")`,
          `delete_key(attributes, "process.executable.path")`,
          `delete_key(attributes, "process.command_line")`,
          `delete_key(attributes, "process.command_args")`,
          `delete_key(attributes, "process.owner")`,
          `delete_key(attributes, "process.runtime.version")`,
          `delete_key(attributes, "process.runtime.description")`,
          `delete_key(attributes, "telemetry.sdk.version")`,
          `delete_key(attributes, "telemetry.sdk.language")`,
          `delete_key(attributes, "host.ip")`,
          `delete_key(attributes, "host.mac")`,
          `delete_key(attributes, "k8s.pod.start_time")`,
          `delete_key(attributes, "k8s.pod.uid")`,
          `delete_key(attributes, "container.image.id")`,
          `delete_key(attributes, "container.image.repo_digests")`,
          `delete_key(attributes, "os.description")`,
          `delete_key(attributes, "os.build_id")`,
        ]
      }
    
      metric_statements {
        context = "datapoint"
        statements = [
          `set(attributes["cluster"], "my-cluster")`,
          `set(attributes["k8s.cluster.name"], "my-cluster")`,
          `set(resource.attributes["deployment.environment"], attributes["deployment_environment"] ) where resource.attributes["deployment.environment"] == nil and attributes["deployment_environment"] != nil`,
          `delete_key(attributes, "deployment_environment") where attributes["deployment_environment"] == resource.attributes["deployment.environment"]`,
          `set(resource.attributes["deployment.environment.name"], attributes["deployment_environment_name"] ) where resource.attributes["deployment.environment.name"] == nil and attributes["deployment_environment_name"] != nil`,
          `delete_key(attributes, "deployment_environment_name") where attributes["deployment_environment_name"] == resource.attributes["deployment.environment.name"]`,
          `set(resource.attributes["service.name"], attributes["service_name"] ) where resource.attributes["service.name"] == nil and attributes["service_name"] != nil`,
          `delete_key(attributes, "service_name") where attributes["service_name"] == resource.attributes["service.name"]`,
          `set(resource.attributes["service.namespace"], attributes["service_namespace"] ) where resource.attributes["service.namespace"] == nil and attributes["service_namespace"] != nil`,
          `delete_key(attributes, "service_namespace") where attributes["service_namespace"] == resource.attributes["service.namespace"]`,
        ]
      }
      log_statements {
        context = "resource"
        statements = [
          `set(attributes["cluster"], "my-cluster")`,
          `set(attributes["k8s.cluster.name"], "my-cluster")`,
          `delete_key(attributes, "process.pid")`,
          `delete_key(attributes, "process.parent_pid")`,
          `delete_key(attributes, "process.executable.path")`,
          `delete_key(attributes, "process.command_line")`,
          `delete_key(attributes, "process.command_args")`,
          `delete_key(attributes, "process.owner")`,
          `delete_key(attributes, "process.runtime.version")`,
          `delete_key(attributes, "process.runtime.description")`,
          `delete_key(attributes, "telemetry.sdk.version")`,
          `delete_key(attributes, "telemetry.sdk.language")`,
          `delete_key(attributes, "host.ip")`,
          `delete_key(attributes, "host.mac")`,
          `delete_key(attributes, "k8s.pod.start_time")`,
          `delete_key(attributes, "k8s.pod.uid")`,
          `delete_key(attributes, "container.image.id")`,
          `delete_key(attributes, "container.image.repo_digests")`,
          `delete_key(attributes, "os.description")`,
          `delete_key(attributes, "os.build_id")`,
        ]
      }
    
      log_statements {
        context = "log"
        statements = [
          `delete_key(attributes, "loki.attribute.labels")`,
          `delete_key(attributes, "loki.resource.labels")`,
          `set(resource.attributes["k8s.container.name"], attributes["container"] ) where resource.attributes["k8s.container.name"] == nil and attributes["container"] != nil`,
          `delete_key(attributes, "container") where attributes["container"] == resource.attributes["k8s.container.name"]`,
          `set(resource.attributes["k8s.cronjob.name"], attributes["cronjob"] ) where resource.attributes["k8s.cronjob.name"] == nil and attributes["cronjob"] != nil`,
          `delete_key(attributes, "cronjob") where attributes["cronjob"] == resource.attributes["k8s.cronjob.name"]`,
          `set(resource.attributes["k8s.daemonset.name"], attributes["daemonset"] ) where resource.attributes["k8s.daemonset.name"] == nil and attributes["daemonset"] != nil`,
          `delete_key(attributes, "daemonset") where attributes["daemonset"] == resource.attributes["k8s.daemonset.name"]`,
          `set(resource.attributes["k8s.deployment.name"], attributes["deployment"] ) where resource.attributes["k8s.deployment.name"] == nil and attributes["deployment"] != nil`,
          `delete_key(attributes, "deployment") where attributes["deployment"] == resource.attributes["k8s.deployment.name"]`,
          `set(resource.attributes["deployment.environment"], attributes["deployment_environment"] ) where resource.attributes["deployment.environment"] == nil and attributes["deployment_environment"] != nil`,
          `delete_key(attributes, "deployment_environment") where attributes["deployment_environment"] == resource.attributes["deployment.environment"]`,
          `set(resource.attributes["deployment.environment.name"], attributes["deployment_environment_name"] ) where resource.attributes["deployment.environment.name"] == nil and attributes["deployment_environment_name"] != nil`,
          `delete_key(attributes, "deployment_environment_name") where attributes["deployment_environment_name"] == resource.attributes["deployment.environment.name"]`,
          `set(resource.attributes["k8s.job.name"], attributes["job_name"] ) where resource.attributes["k8s.job.name"] == nil and attributes["job_name"] != nil`,
          `delete_key(attributes, "job_name") where attributes["job_name"] == resource.attributes["k8s.job.name"]`,
          `set(resource.attributes["k8s.namespace.name"], attributes["namespace"] ) where resource.attributes["k8s.namespace.name"] == nil and attributes["namespace"] != nil`,
          `delete_key(attributes, "namespace") where attributes["namespace"] == resource.attributes["k8s.namespace.name"]`,
          `set(resource.attributes["k8s.pod.name"], attributes["pod"] ) where resource.attributes["k8s.pod.name"] == nil and attributes["pod"] != nil`,
          `delete_key(attributes, "pod") where attributes["pod"] == resource.attributes["k8s.pod.name"]`,
          `set(resource.attributes["k8s.replicaset.name"], attributes["replicaset"] ) where resource.attributes["k8s.replicaset.name"] == nil and attributes["replicaset"] != nil`,
          `delete_key(attributes, "replicaset") where attributes["replicaset"] == resource.attributes["k8s.replicaset.name"]`,
          `set(resource.attributes["service.name"], attributes["service_name"] ) where resource.attributes["service.name"] == nil and attributes["service_name"] != nil`,
          `delete_key(attributes, "service_name") where attributes["service_name"] == resource.attributes["service.name"]`,
          `set(resource.attributes["service.namespace"], attributes["service_namespace"] ) where resource.attributes["service.namespace"] == nil and attributes["service_namespace"] != nil`,
          `delete_key(attributes, "service_namespace") where attributes["service_namespace"] == resource.attributes["service.namespace"]`,
          `set(resource.attributes["k8s.statefulset.name"], attributes["statefulset"] ) where resource.attributes["k8s.statefulset.name"] == nil and attributes["statefulset"] != nil`,
          `delete_key(attributes, "statefulset") where attributes["statefulset"] == resource.attributes["k8s.statefulset.name"]`,
        ]
      }
    
      trace_statements {
        context = "resource"
        statements = [
          `set(attributes["cluster"], "my-cluster")`,
          `set(attributes["k8s.cluster.name"], "my-cluster")`,
          `delete_key(attributes, "process.pid")`,
          `delete_key(attributes, "process.parent_pid")`,
          `delete_key(attributes, "process.executable.path")`,
          `delete_key(attributes, "process.command_line")`,
          `delete_key(attributes, "process.command_args")`,
          `delete_key(attributes, "process.owner")`,
          `delete_key(attributes, "process.runtime.version")`,
          `delete_key(attributes, "process.runtime.description")`,
          `delete_key(attributes, "telemetry.sdk.version")`,
          `delete_key(attributes, "telemetry.sdk.language")`,
          `delete_key(attributes, "host.ip")`,
          `delete_key(attributes, "host.mac")`,
          `delete_key(attributes, "k8s.pod.start_time")`,
          `delete_key(attributes, "k8s.pod.uid")`,
          `delete_key(attributes, "container.image.id")`,
          `delete_key(attributes, "container.image.repo_digests")`,
          `delete_key(attributes, "os.description")`,
          `delete_key(attributes, "os.build_id")`,
        ]
      }
    
      output {
        metrics = [otelcol.processor.batch.gc_otlp_endpoint.input]
        logs = [otelcol.processor.batch.gc_otlp_endpoint.input]
        traces = [otelcol.processor.batch.gc_otlp_endpoint.input]
      }
    }
    
    otelcol.processor.batch "gc_otlp_endpoint" {
      timeout = "2s"
      send_batch_size = 8192
      send_batch_max_size = 0
    
      output {
        metrics = [otelcol.exporter.otlphttp.gc_otlp_endpoint.input]
        logs = [otelcol.exporter.otlphttp.gc_otlp_endpoint.input]
        traces = [otelcol.exporter.otlphttp.gc_otlp_endpoint.input]
      }
    }
    otelcol.exporter.otlphttp "gc_otlp_endpoint" {
      client {
        endpoint = "https://otlp-gateway-prod-eu-west-2.grafana.net./otlp"
        auth = otelcol.auth.basic.gc_otlp_endpoint.handler
        tls {
          insecure = false
          insecure_skip_verify = false
        }
      }
    
      retry_on_failure {
        enabled = true
        initial_interval = "5s"
        max_interval = "30s"
        max_elapsed_time = "5m"
      }
    }
    
    otelcol.auth.basic "gc_otlp_endpoint" {
      username = convert.nonsensitive(remote.kubernetes.secret.gc_otlp_endpoint.data["username"])
      password = remote.kubernetes.secret.gc_otlp_endpoint.data["password"]
    }
    
    remote.kubernetes.secret "gc_otlp_endpoint" {
      name      = "gc-otlp-endpoint-grafana-k8s-monitoring"
      namespace = "default"
    }
---
# Source: k8s-monitoring/templates/alloy-config.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: grafana-k8s-monitoring-alloy-profiles
  namespace: grafana
data:
  config.alloy: |
    // Feature: Profiling
    declare "profiling" {
      argument "profiles_destinations" {
        comment = "Must be a list of profile destinations where collected profiles should be forwarded to"
      }
      // Profiles: eBPF
      discovery.kubernetes "ebpf_pods" {
        role = "pod"
        selectors {
          role = "pod"
          field = "spec.nodeName=" + sys.env("HOSTNAME")
        }
      }
    
      discovery.relabel "ebpf_pods" {
        targets = discovery.kubernetes.ebpf_pods.targets
        rule {
          source_labels = ["__meta_kubernetes_pod_phase"]
          regex = "Succeeded|Failed|Completed"
          action = "drop"
        }
        rule {
          source_labels = ["__meta_kubernetes_pod_annotation_profiles_grafana_com_cpu_ebpf_enabled"]
          regex         = "true"
          action        = "keep"
        }
        rule {
          source_labels = ["__meta_kubernetes_namespace"]
          target_label = "namespace"
        }
        rule {
          source_labels = ["__meta_kubernetes_pod_name"]
          target_label = "pod"
        }
        rule {
          source_labels = ["__meta_kubernetes_pod_node_name"]
          target_label = "node"
        }
        rule {
          source_labels = ["__meta_kubernetes_pod_container_name"]
          target_label = "container"
        }
        // provide arbitrary service_name label, otherwise it will be set to {__meta_kubernetes_namespace}/{__meta_kubernetes_pod_container_name}
        rule {
          source_labels = ["__meta_kubernetes_namespace", "__meta_kubernetes_pod_container_name"]
          separator = "@"
          regex = "(.*)@(.*)"
          replacement = "ebpf/${1}/${2}"
          target_label = "service_name"
        }
        rule {
          replacement = "alloy/pyroscope.ebpf"
          target_label = "source"
        }
      }
    
      pyroscope.ebpf "ebpf_pods" {
        targets = discovery.relabel.ebpf_pods.output
        demangle = "none"
        forward_to = argument.profiles_destinations.value
      }
      // Profiles: Java
      discovery.kubernetes "java_pods" {
        role = "pod"
        selectors {
          role = "pod"
          field = "spec.nodeName=" + sys.env("HOSTNAME")
        }
      }
    
      discovery.relabel "potential_java_pods" {
        targets = discovery.kubernetes.java_pods.targets
        rule {
          source_labels = ["__meta_kubernetes_pod_phase"]
          regex         = "Succeeded|Failed|Completed"
          action        = "drop"
        }
        rule {
          source_labels = ["__meta_kubernetes_pod_annotation_profiles_grafana_com_java_enabled"]
          regex         = "true"
          action        = "keep"
        }
      }
    
      discovery.process "java_pods" {
        join = discovery.relabel.potential_java_pods.output
      }
    
      discovery.relabel "java_pods" {
        targets = discovery.process.java_pods.targets
        rule {
          source_labels = ["__meta_process_exe"]
          action = "keep"
          regex = ".*/java$"
        }
        rule {
          source_labels = ["__meta_kubernetes_namespace"]
          target_label = "namespace"
        }
        rule {
          source_labels = ["__meta_kubernetes_pod_name"]
          target_label = "pod"
        }
        rule {
          source_labels = ["__meta_kubernetes_pod_node_name"]
          target_label = "node"
        }
        rule {
          source_labels = ["__meta_kubernetes_pod_container_name"]
          target_label = "container"
        }
        rule {
          replacement = "alloy/pyroscope.java"
          target_label = "source"
        }
      }
    
      pyroscope.java "java_pods" {
        targets = discovery.relabel.java_pods.output
        profiling_config {
          interval = "60s"
          alloc = "512k"
          cpu = true
          sample_rate = 100
          lock = "10ms"
        }
        forward_to = argument.profiles_destinations.value
      }
      // Profiles: pprof
      discovery.kubernetes "pprof_pods" {
        role = "pod"
        selectors {
          role = "pod"
          field = "spec.nodeName=" + sys.env("HOSTNAME")
        }
      }
    
      discovery.relabel "pprof_pods" {
        targets = discovery.kubernetes.pprof_pods.targets
        rule {
          action        = "drop"
          source_labels = ["__meta_kubernetes_pod_phase"]
          regex         = "Pending|Succeeded|Failed|Completed"
        }
    
        rule {
          regex  = "__meta_kubernetes_pod_label_(.+)"
          action = "labelmap"
        }
        rule {
          source_labels = ["__meta_kubernetes_namespace"]
          target_label  = "namespace"
        }
        rule {
          source_labels = ["__meta_kubernetes_pod_name"]
          target_label  = "pod"
        }
        rule {
          source_labels = ["__meta_kubernetes_pod_container_name"]
          target_label  = "container"
        }
        rule {
          replacement = "alloy/pyroscope.pprof"
          target_label = "source"
        }
      }
    
      discovery.relabel "pprof_pods_block_default_name" {
        targets = discovery.relabel.pprof_pods.output
    
        // Keep only pods with the scrape annotation set
        rule {
          source_labels = ["__meta_kubernetes_pod_annotation_profiles_grafana_com_block_scrape"]
          regex         = "true"
          action        = "keep"
        }
    
        // Rules to choose the right container
        rule {
          source_labels = ["container"]
          target_label = "__tmp_container"
        }
        rule {
          source_labels = ["__meta_kubernetes_pod_annotation_profiles_grafana_com_block_container"]
          regex = "(.+)"
          target_label = "__tmp_container"
        }
        rule {
          source_labels = ["container"]
          action = "keepequal"
          target_label = "__tmp_container"
        }
        rule {
          action = "labeldrop"
          regex = "__tmp_container"
        }
    
        // Rules to choose the right port by name
        // The discovery generates a target for each declared container port of the pod.
        // If the portName annotation has value, keep only the target where the port name matches the one of the annotation.
        rule {
          source_labels = ["__meta_kubernetes_pod_container_port_name"]
          target_label = "__tmp_port"
        }
        rule {
          source_labels = ["__meta_kubernetes_pod_annotation_profiles_grafana_com_block_port_name"]
          regex = "(.+)"
          target_label = "__tmp_port"
        }
        rule {
          source_labels = ["__meta_kubernetes_pod_container_port_name"]
          action = "keepequal"
          target_label = "__tmp_port"
        }
        rule {
          action = "labeldrop"
          regex = "__tmp_port"
        }
    
        // If the portNumber annotation has a value, override the target address to use it, regardless whether it is
        // one of the declared ports on that Pod.
        rule {
          source_labels = ["__meta_kubernetes_pod_annotation_profiles_grafana_com_block_port", "__meta_kubernetes_pod_ip"]
          regex = "(\\d+);(([A-Fa-f0-9]{1,4}::?){1,7}[A-Fa-f0-9]{1,4})"
          replacement = "[$2]:$1" // IPv6
          target_label = "__address__"
        }
        rule {
          source_labels = ["__meta_kubernetes_pod_annotation_profiles_grafana_com_block_port", "__meta_kubernetes_pod_ip"]
          regex = "(\\d+);((([0-9]+?)(\\.|$)){4})" // IPv4, takes priority over IPv6 when both exists
          replacement = "$2:$1"
          target_label = "__address__"
        }
    
        rule {
          source_labels = ["__meta_kubernetes_pod_annotation_profiles_grafana_com_block_scheme"]
          regex         = "(https?)"
          target_label  = "__scheme__"
        }
        rule {
          source_labels = ["__meta_kubernetes_pod_annotation_profiles_grafana_com_block_path"]
          regex         = "(.+)"
          target_label  = "__profile_path__"
        }
      }
    
      pyroscope.scrape "pyroscope_scrape_block" {
        targets = discovery.relabel.pprof_pods_block_default_name.output
    
        bearer_token_file = "/var/run/secrets/kubernetes.io/serviceaccount/token"
        profiling_config {
          profile.block {
            enabled = true
          }
          profile.process_cpu {
            enabled = false
          }
          profile.fgprof {
            enabled = false
          }
          profile.godeltaprof_block {
            enabled = false
          }
          profile.godeltaprof_memory {
            enabled = false
          }
          profile.godeltaprof_mutex {
            enabled = false
          }
          profile.goroutine {
            enabled = false
          }
          profile.memory {
            enabled = false
          }
          profile.mutex {
            enabled = false
          }
        }
    
        scrape_interval = "15s"
        scrape_timeout = "18s"
    
        forward_to = argument.profiles_destinations.value
      }
      discovery.relabel "pprof_pods_cpu_default_name" {
        targets = discovery.relabel.pprof_pods.output
    
        // Keep only pods with the scrape annotation set
        rule {
          source_labels = ["__meta_kubernetes_pod_annotation_profiles_grafana_com_cpu_scrape"]
          regex         = "true"
          action        = "keep"
        }
    
        // Rules to choose the right container
        rule {
          source_labels = ["container"]
          target_label = "__tmp_container"
        }
        rule {
          source_labels = ["__meta_kubernetes_pod_annotation_profiles_grafana_com_cpu_container"]
          regex = "(.+)"
          target_label = "__tmp_container"
        }
        rule {
          source_labels = ["container"]
          action = "keepequal"
          target_label = "__tmp_container"
        }
        rule {
          action = "labeldrop"
          regex = "__tmp_container"
        }
    
        // Rules to choose the right port by name
        // The discovery generates a target for each declared container port of the pod.
        // If the portName annotation has value, keep only the target where the port name matches the one of the annotation.
        rule {
          source_labels = ["__meta_kubernetes_pod_container_port_name"]
          target_label = "__tmp_port"
        }
        rule {
          source_labels = ["__meta_kubernetes_pod_annotation_profiles_grafana_com_cpu_port_name"]
          regex = "(.+)"
          target_label = "__tmp_port"
        }
        rule {
          source_labels = ["__meta_kubernetes_pod_container_port_name"]
          action = "keepequal"
          target_label = "__tmp_port"
        }
        rule {
          action = "labeldrop"
          regex = "__tmp_port"
        }
    
        // If the portNumber annotation has a value, override the target address to use it, regardless whether it is
        // one of the declared ports on that Pod.
        rule {
          source_labels = ["__meta_kubernetes_pod_annotation_profiles_grafana_com_cpu_port", "__meta_kubernetes_pod_ip"]
          regex = "(\\d+);(([A-Fa-f0-9]{1,4}::?){1,7}[A-Fa-f0-9]{1,4})"
          replacement = "[$2]:$1" // IPv6
          target_label = "__address__"
        }
        rule {
          source_labels = ["__meta_kubernetes_pod_annotation_profiles_grafana_com_cpu_port", "__meta_kubernetes_pod_ip"]
          regex = "(\\d+);((([0-9]+?)(\\.|$)){4})" // IPv4, takes priority over IPv6 when both exists
          replacement = "$2:$1"
          target_label = "__address__"
        }
    
        rule {
          source_labels = ["__meta_kubernetes_pod_annotation_profiles_grafana_com_cpu_scheme"]
          regex         = "(https?)"
          target_label  = "__scheme__"
        }
        rule {
          source_labels = ["__meta_kubernetes_pod_annotation_profiles_grafana_com_cpu_path"]
          regex         = "(.+)"
          target_label  = "__profile_path__"
        }
      }
    
      pyroscope.scrape "pyroscope_scrape_cpu" {
        targets = discovery.relabel.pprof_pods_cpu_default_name.output
    
        bearer_token_file = "/var/run/secrets/kubernetes.io/serviceaccount/token"
        profiling_config {
          profile.block {
            enabled = false
          }
          profile.process_cpu {
            enabled = true
          }
          profile.fgprof {
            enabled = false
          }
          profile.godeltaprof_block {
            enabled = false
          }
          profile.godeltaprof_memory {
            enabled = false
          }
          profile.godeltaprof_mutex {
            enabled = false
          }
          profile.goroutine {
            enabled = false
          }
          profile.memory {
            enabled = false
          }
          profile.mutex {
            enabled = false
          }
        }
    
        scrape_interval = "15s"
        scrape_timeout = "18s"
    
        forward_to = argument.profiles_destinations.value
      }
      discovery.relabel "pprof_pods_fgprof_default_name" {
        targets = discovery.relabel.pprof_pods.output
    
        // Keep only pods with the scrape annotation set
        rule {
          source_labels = ["__meta_kubernetes_pod_annotation_profiles_grafana_com_fgprof_scrape"]
          regex         = "true"
          action        = "keep"
        }
    
        // Rules to choose the right container
        rule {
          source_labels = ["container"]
          target_label = "__tmp_container"
        }
        rule {
          source_labels = ["__meta_kubernetes_pod_annotation_profiles_grafana_com_fgprof_container"]
          regex = "(.+)"
          target_label = "__tmp_container"
        }
        rule {
          source_labels = ["container"]
          action = "keepequal"
          target_label = "__tmp_container"
        }
        rule {
          action = "labeldrop"
          regex = "__tmp_container"
        }
    
        // Rules to choose the right port by name
        // The discovery generates a target for each declared container port of the pod.
        // If the portName annotation has value, keep only the target where the port name matches the one of the annotation.
        rule {
          source_labels = ["__meta_kubernetes_pod_container_port_name"]
          target_label = "__tmp_port"
        }
        rule {
          source_labels = ["__meta_kubernetes_pod_annotation_profiles_grafana_com_fgprof_port_name"]
          regex = "(.+)"
          target_label = "__tmp_port"
        }
        rule {
          source_labels = ["__meta_kubernetes_pod_container_port_name"]
          action = "keepequal"
          target_label = "__tmp_port"
        }
        rule {
          action = "labeldrop"
          regex = "__tmp_port"
        }
    
        // If the portNumber annotation has a value, override the target address to use it, regardless whether it is
        // one of the declared ports on that Pod.
        rule {
          source_labels = ["__meta_kubernetes_pod_annotation_profiles_grafana_com_fgprof_port", "__meta_kubernetes_pod_ip"]
          regex = "(\\d+);(([A-Fa-f0-9]{1,4}::?){1,7}[A-Fa-f0-9]{1,4})"
          replacement = "[$2]:$1" // IPv6
          target_label = "__address__"
        }
        rule {
          source_labels = ["__meta_kubernetes_pod_annotation_profiles_grafana_com_fgprof_port", "__meta_kubernetes_pod_ip"]
          regex = "(\\d+);((([0-9]+?)(\\.|$)){4})" // IPv4, takes priority over IPv6 when both exists
          replacement = "$2:$1"
          target_label = "__address__"
        }
    
        rule {
          source_labels = ["__meta_kubernetes_pod_annotation_profiles_grafana_com_fgprof_scheme"]
          regex         = "(https?)"
          target_label  = "__scheme__"
        }
        rule {
          source_labels = ["__meta_kubernetes_pod_annotation_profiles_grafana_com_fgprof_path"]
          regex         = "(.+)"
          target_label  = "__profile_path__"
        }
      }
    
      pyroscope.scrape "pyroscope_scrape_fgprof" {
        targets = discovery.relabel.pprof_pods_fgprof_default_name.output
    
        bearer_token_file = "/var/run/secrets/kubernetes.io/serviceaccount/token"
        profiling_config {
          profile.block {
            enabled = false
          }
          profile.process_cpu {
            enabled = false
          }
          profile.fgprof {
            enabled = true
          }
          profile.godeltaprof_block {
            enabled = false
          }
          profile.godeltaprof_memory {
            enabled = false
          }
          profile.godeltaprof_mutex {
            enabled = false
          }
          profile.goroutine {
            enabled = false
          }
          profile.memory {
            enabled = false
          }
          profile.mutex {
            enabled = false
          }
        }
    
        scrape_interval = "15s"
        scrape_timeout = "18s"
    
        forward_to = argument.profiles_destinations.value
      }
      discovery.relabel "pprof_pods_goroutine_default_name" {
        targets = discovery.relabel.pprof_pods.output
    
        // Keep only pods with the scrape annotation set
        rule {
          source_labels = ["__meta_kubernetes_pod_annotation_profiles_grafana_com_goroutine_scrape"]
          regex         = "true"
          action        = "keep"
        }
    
        // Rules to choose the right container
        rule {
          source_labels = ["container"]
          target_label = "__tmp_container"
        }
        rule {
          source_labels = ["__meta_kubernetes_pod_annotation_profiles_grafana_com_goroutine_container"]
          regex = "(.+)"
          target_label = "__tmp_container"
        }
        rule {
          source_labels = ["container"]
          action = "keepequal"
          target_label = "__tmp_container"
        }
        rule {
          action = "labeldrop"
          regex = "__tmp_container"
        }
    
        // Rules to choose the right port by name
        // The discovery generates a target for each declared container port of the pod.
        // If the portName annotation has value, keep only the target where the port name matches the one of the annotation.
        rule {
          source_labels = ["__meta_kubernetes_pod_container_port_name"]
          target_label = "__tmp_port"
        }
        rule {
          source_labels = ["__meta_kubernetes_pod_annotation_profiles_grafana_com_goroutine_port_name"]
          regex = "(.+)"
          target_label = "__tmp_port"
        }
        rule {
          source_labels = ["__meta_kubernetes_pod_container_port_name"]
          action = "keepequal"
          target_label = "__tmp_port"
        }
        rule {
          action = "labeldrop"
          regex = "__tmp_port"
        }
    
        // If the portNumber annotation has a value, override the target address to use it, regardless whether it is
        // one of the declared ports on that Pod.
        rule {
          source_labels = ["__meta_kubernetes_pod_annotation_profiles_grafana_com_goroutine_port", "__meta_kubernetes_pod_ip"]
          regex = "(\\d+);(([A-Fa-f0-9]{1,4}::?){1,7}[A-Fa-f0-9]{1,4})"
          replacement = "[$2]:$1" // IPv6
          target_label = "__address__"
        }
        rule {
          source_labels = ["__meta_kubernetes_pod_annotation_profiles_grafana_com_goroutine_port", "__meta_kubernetes_pod_ip"]
          regex = "(\\d+);((([0-9]+?)(\\.|$)){4})" // IPv4, takes priority over IPv6 when both exists
          replacement = "$2:$1"
          target_label = "__address__"
        }
    
        rule {
          source_labels = ["__meta_kubernetes_pod_annotation_profiles_grafana_com_goroutine_scheme"]
          regex         = "(https?)"
          target_label  = "__scheme__"
        }
        rule {
          source_labels = ["__meta_kubernetes_pod_annotation_profiles_grafana_com_goroutine_path"]
          regex         = "(.+)"
          target_label  = "__profile_path__"
        }
      }
    
      pyroscope.scrape "pyroscope_scrape_goroutine" {
        targets = discovery.relabel.pprof_pods_goroutine_default_name.output
    
        bearer_token_file = "/var/run/secrets/kubernetes.io/serviceaccount/token"
        profiling_config {
          profile.block {
            enabled = false
          }
          profile.process_cpu {
            enabled = false
          }
          profile.fgprof {
            enabled = false
          }
          profile.godeltaprof_block {
            enabled = false
          }
          profile.godeltaprof_memory {
            enabled = false
          }
          profile.godeltaprof_mutex {
            enabled = false
          }
          profile.goroutine {
            enabled = true
          }
          profile.memory {
            enabled = false
          }
          profile.mutex {
            enabled = false
          }
        }
    
        scrape_interval = "15s"
        scrape_timeout = "18s"
    
        forward_to = argument.profiles_destinations.value
      }
      discovery.relabel "pprof_pods_memory_default_name" {
        targets = discovery.relabel.pprof_pods.output
    
        // Keep only pods with the scrape annotation set
        rule {
          source_labels = ["__meta_kubernetes_pod_annotation_profiles_grafana_com_memory_scrape"]
          regex         = "true"
          action        = "keep"
        }
    
        // Rules to choose the right container
        rule {
          source_labels = ["container"]
          target_label = "__tmp_container"
        }
        rule {
          source_labels = ["__meta_kubernetes_pod_annotation_profiles_grafana_com_memory_container"]
          regex = "(.+)"
          target_label = "__tmp_container"
        }
        rule {
          source_labels = ["container"]
          action = "keepequal"
          target_label = "__tmp_container"
        }
        rule {
          action = "labeldrop"
          regex = "__tmp_container"
        }
    
        // Rules to choose the right port by name
        // The discovery generates a target for each declared container port of the pod.
        // If the portName annotation has value, keep only the target where the port name matches the one of the annotation.
        rule {
          source_labels = ["__meta_kubernetes_pod_container_port_name"]
          target_label = "__tmp_port"
        }
        rule {
          source_labels = ["__meta_kubernetes_pod_annotation_profiles_grafana_com_memory_port_name"]
          regex = "(.+)"
          target_label = "__tmp_port"
        }
        rule {
          source_labels = ["__meta_kubernetes_pod_container_port_name"]
          action = "keepequal"
          target_label = "__tmp_port"
        }
        rule {
          action = "labeldrop"
          regex = "__tmp_port"
        }
    
        // If the portNumber annotation has a value, override the target address to use it, regardless whether it is
        // one of the declared ports on that Pod.
        rule {
          source_labels = ["__meta_kubernetes_pod_annotation_profiles_grafana_com_memory_port", "__meta_kubernetes_pod_ip"]
          regex = "(\\d+);(([A-Fa-f0-9]{1,4}::?){1,7}[A-Fa-f0-9]{1,4})"
          replacement = "[$2]:$1" // IPv6
          target_label = "__address__"
        }
        rule {
          source_labels = ["__meta_kubernetes_pod_annotation_profiles_grafana_com_memory_port", "__meta_kubernetes_pod_ip"]
          regex = "(\\d+);((([0-9]+?)(\\.|$)){4})" // IPv4, takes priority over IPv6 when both exists
          replacement = "$2:$1"
          target_label = "__address__"
        }
    
        rule {
          source_labels = ["__meta_kubernetes_pod_annotation_profiles_grafana_com_memory_scheme"]
          regex         = "(https?)"
          target_label  = "__scheme__"
        }
        rule {
          source_labels = ["__meta_kubernetes_pod_annotation_profiles_grafana_com_memory_path"]
          regex         = "(.+)"
          target_label  = "__profile_path__"
        }
      }
    
      pyroscope.scrape "pyroscope_scrape_memory" {
        targets = discovery.relabel.pprof_pods_memory_default_name.output
    
        bearer_token_file = "/var/run/secrets/kubernetes.io/serviceaccount/token"
        profiling_config {
          profile.block {
            enabled = false
          }
          profile.process_cpu {
            enabled = false
          }
          profile.fgprof {
            enabled = false
          }
          profile.godeltaprof_block {
            enabled = false
          }
          profile.godeltaprof_memory {
            enabled = false
          }
          profile.godeltaprof_mutex {
            enabled = false
          }
          profile.goroutine {
            enabled = false
          }
          profile.memory {
            enabled = true
          }
          profile.mutex {
            enabled = false
          }
        }
    
        scrape_interval = "15s"
        scrape_timeout = "18s"
    
        forward_to = argument.profiles_destinations.value
      }
      discovery.relabel "pprof_pods_mutex_default_name" {
        targets = discovery.relabel.pprof_pods.output
    
        // Keep only pods with the scrape annotation set
        rule {
          source_labels = ["__meta_kubernetes_pod_annotation_profiles_grafana_com_mutex_scrape"]
          regex         = "true"
          action        = "keep"
        }
    
        // Rules to choose the right container
        rule {
          source_labels = ["container"]
          target_label = "__tmp_container"
        }
        rule {
          source_labels = ["__meta_kubernetes_pod_annotation_profiles_grafana_com_mutex_container"]
          regex = "(.+)"
          target_label = "__tmp_container"
        }
        rule {
          source_labels = ["container"]
          action = "keepequal"
          target_label = "__tmp_container"
        }
        rule {
          action = "labeldrop"
          regex = "__tmp_container"
        }
    
        // Rules to choose the right port by name
        // The discovery generates a target for each declared container port of the pod.
        // If the portName annotation has value, keep only the target where the port name matches the one of the annotation.
        rule {
          source_labels = ["__meta_kubernetes_pod_container_port_name"]
          target_label = "__tmp_port"
        }
        rule {
          source_labels = ["__meta_kubernetes_pod_annotation_profiles_grafana_com_mutex_port_name"]
          regex = "(.+)"
          target_label = "__tmp_port"
        }
        rule {
          source_labels = ["__meta_kubernetes_pod_container_port_name"]
          action = "keepequal"
          target_label = "__tmp_port"
        }
        rule {
          action = "labeldrop"
          regex = "__tmp_port"
        }
    
        // If the portNumber annotation has a value, override the target address to use it, regardless whether it is
        // one of the declared ports on that Pod.
        rule {
          source_labels = ["__meta_kubernetes_pod_annotation_profiles_grafana_com_mutex_port", "__meta_kubernetes_pod_ip"]
          regex = "(\\d+);(([A-Fa-f0-9]{1,4}::?){1,7}[A-Fa-f0-9]{1,4})"
          replacement = "[$2]:$1" // IPv6
          target_label = "__address__"
        }
        rule {
          source_labels = ["__meta_kubernetes_pod_annotation_profiles_grafana_com_mutex_port", "__meta_kubernetes_pod_ip"]
          regex = "(\\d+);((([0-9]+?)(\\.|$)){4})" // IPv4, takes priority over IPv6 when both exists
          replacement = "$2:$1"
          target_label = "__address__"
        }
    
        rule {
          source_labels = ["__meta_kubernetes_pod_annotation_profiles_grafana_com_mutex_scheme"]
          regex         = "(https?)"
          target_label  = "__scheme__"
        }
        rule {
          source_labels = ["__meta_kubernetes_pod_annotation_profiles_grafana_com_mutex_path"]
          regex         = "(.+)"
          target_label  = "__profile_path__"
        }
      }
    
      pyroscope.scrape "pyroscope_scrape_mutex" {
        targets = discovery.relabel.pprof_pods_mutex_default_name.output
    
        bearer_token_file = "/var/run/secrets/kubernetes.io/serviceaccount/token"
        profiling_config {
          profile.block {
            enabled = false
          }
          profile.process_cpu {
            enabled = false
          }
          profile.fgprof {
            enabled = false
          }
          profile.godeltaprof_block {
            enabled = false
          }
          profile.godeltaprof_memory {
            enabled = false
          }
          profile.godeltaprof_mutex {
            enabled = false
          }
          profile.goroutine {
            enabled = false
          }
          profile.memory {
            enabled = false
          }
          profile.mutex {
            enabled = true
          }
        }
    
        scrape_interval = "15s"
        scrape_timeout = "18s"
    
        forward_to = argument.profiles_destinations.value
      }
    }
    profiling "feature" {
      profiles_destinations = [
        pyroscope.write.grafana_cloud_profiles.receiver,
      ]
    }
    
    
    remote.kubernetes.secret "alloy_profiles_remote_cfg" {
      name      = "alloy-profiles-remote-cfg-grafana-k8s-monitoring"
      namespace = "default"
    }
    
    remotecfg {
      id = sys.env("GCLOUD_FM_COLLECTOR_ID")
      url = "https://fleet-management-prod-011.grafana.net"
      basic_auth {
        username = convert.nonsensitive(remote.kubernetes.secret.alloy_profiles_remote_cfg.data["username"])
        password = remote.kubernetes.secret.alloy_profiles_remote_cfg.data["password"]
      }
      tls_config {
        insecure_skip_verify = false
      }
      poll_frequency = "5m"
      attributes = {
        "cluster" = "my-cluster",
        "namespace" = "default",
        "platform" = "kubernetes",
        "release" = "grafana-k8s-monitoring",
        "source" = "k8s-monitoring",
        "sourceVersion" = "3.4.2",
        "workloadName" = "alloy-profiles",
        "workloadType" = "daemonset",
      }
    }
    
    // Destination: grafana-cloud-profiles (pyroscope)
    pyroscope.write "grafana_cloud_profiles" {
      endpoint {
        url = "https://profiles-prod-002.grafana.net.:443"
        headers = {
        }
        basic_auth {
          username = convert.nonsensitive(remote.kubernetes.secret.grafana_cloud_profiles.data["username"])
          password = remote.kubernetes.secret.grafana_cloud_profiles.data["password"]
        }
        tls_config {
          insecure_skip_verify = false
        }
        min_backoff_period = "500ms"
        max_backoff_period = "5m"
        max_backoff_retries = "10"
      }
    
      external_labels = {
        "cluster" = "my-cluster",
        "k8s_cluster_name" = "my-cluster",
      }
    }
    
    remote.kubernetes.secret "grafana_cloud_profiles" {
      name      = "grafana-cloud-profiles-grafana-k8s-monitoring"
      namespace = "default"
    }
